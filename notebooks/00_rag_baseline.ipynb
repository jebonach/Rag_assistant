{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d65fc778",
      "metadata": {},
      "source": [
        "# RAG baseline notebook\n",
        "\n",
        "Краткий запуск:\n",
        "1) Создай venv и установи зависимости из requirements.txt\n",
        "2) Скопируй .env.example -> .env и заполни ключи/тарифы при необходимости\n",
        "3) Restart & Run All\n",
        "\n",
        "Настройки:\n",
        "- Меняются в ячейке Config (book_id, top_k, retrieval_mode, флаги rebuild_*)\n",
        "- Модели/ключи и тарифы берутся из .env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b61683a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "PROJECT_DIR = Path(\"..\").resolve()\n",
        "ENV_PATH = PROJECT_DIR / \".env\"\n",
        "if ENV_PATH.exists():\n",
        "    load_dotenv(ENV_PATH)\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "    seed: int = int(os.getenv(\"SEED\", \"42\"))\n",
        "\n",
        "    # paths\n",
        "    project_dir: Path = PROJECT_DIR\n",
        "    data_dir: Path = PROJECT_DIR / \"data\"\n",
        "    indexes_dir: Path = PROJECT_DIR / \"indexes\"\n",
        "    artifacts_dir: Path = PROJECT_DIR / \"artifacts\"\n",
        "\n",
        "    # book/page corpus\n",
        "    book_id: str = os.getenv(\"BOOK_ID\", \"devops_handbook\")\n",
        "    rebuild_pages: bool = os.getenv(\"REBUILD_PAGES\", \"false\").lower() == \"true\"\n",
        "    rebuild_indexes: bool = os.getenv(\"REBUILD_INDEXES\", \"false\").lower() == \"true\"\n",
        "\n",
        "    # retrieval params\n",
        "    retrieval_mode: str = os.getenv(\"RETRIEVAL_MODE\", \"bm25\")  # bm25 | vector | hybrid\n",
        "    top_k: int = int(os.getenv(\"TOP_K\", \"5\"))\n",
        "    vector_top_k: int = int(os.getenv(\"VECTOR_TOP_K\", \"5\"))\n",
        "    rrf_k: int = int(os.getenv(\"RRF_K\", \"60\"))\n",
        "    max_context_chars: int = int(os.getenv(\"MAX_CONTEXT_CHARS\", \"6000\"))\n",
        "\n",
        "    # embeddings / LLM\n",
        "    embed_batch_size: int = int(os.getenv(\"EMBED_BATCH_SIZE\", \"64\"))\n",
        "    embedding_model: str = os.getenv(\"EMBEDDING_MODEL\", \"\")\n",
        "    chat_model: str = os.getenv(\"CHAT_MODEL\", \"\")\n",
        "    openai_base_url: str = os.getenv(\"OPENAI_BASE_URL\", \"\").strip()\n",
        "    llm_temperature: float = float(os.getenv(\"LLM_TEMPERATURE\", \"0.0\"))\n",
        "    llm_max_tokens: int = int(os.getenv(\"LLM_MAX_TOKENS\", \"600\"))\n",
        "\n",
        "    # evaluation\n",
        "    eval_ks: tuple[int, ...] = (3, 5)\n",
        "\n",
        "    # demo\n",
        "    demo_queries: tuple[str, ...] = (\n",
        "        \"что такое RAG\",\n",
        "        \"индексация\",\n",
        "        \"модель\",\n",
        "    )\n",
        "    ask_questions: tuple[str, ...] = (\n",
        "        \"Как в книге формулируется цель DevOps и почему она важна?\",\n",
        "        \"Какие три пути (The Three Ways) описывает автор и в чем их смысл?\",\n",
        "        \"Что такое value stream mapping и для чего он используется?\",\n",
        "    )\n",
        "    demo_snippet_chars: int = int(os.getenv(\"DEMO_SNIPPET_CHARS\", \"180\"))\n",
        "\n",
        "    @property\n",
        "    def book_dir(self) -> Path:\n",
        "        return self.data_dir / \"books\" / self.book_id\n",
        "\n",
        "    @property\n",
        "    def pages_dir(self) -> Path:\n",
        "        return self.book_dir / \"pages\"\n",
        "\n",
        "    @property\n",
        "    def book_txt_path(self) -> Path:\n",
        "        return self.book_dir / \"book.txt\"\n",
        "\n",
        "    @property\n",
        "    def book_md_path(self) -> Path:\n",
        "        return self.book_dir / \"book.md\"\n",
        "\n",
        "    @property\n",
        "    def pages_csv_path(self) -> Path:\n",
        "        return self.artifacts_dir / \"pages.csv\"\n",
        "\n",
        "    @property\n",
        "    def bm25_index_path(self) -> Path:\n",
        "        return self.indexes_dir / \"bm25.pkl\"\n",
        "\n",
        "    @property\n",
        "    def vector_emb_path(self) -> Path:\n",
        "        return self.indexes_dir / \"vector_embeddings.npy\"\n",
        "\n",
        "    @property\n",
        "    def vector_meta_path(self) -> Path:\n",
        "        return self.indexes_dir / \"vector_meta.json\"\n",
        "\n",
        "    @property\n",
        "    def eval_questions_path(self) -> Path:\n",
        "        return self.project_dir / \"eval\" / \"questions.json\"\n",
        "\n",
        "    @property\n",
        "    def eval_out_path(self) -> Path:\n",
        "        return self.artifacts_dir / \"retrieval_eval.csv\"\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefb2bf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
        "OPENAI_BASE_URL = cfg.openai_base_url\n",
        "CHAT_MODEL = cfg.chat_model\n",
        "EMBEDDING_MODEL = cfg.embedding_model\n",
        "\n",
        "embeddings_enabled = bool(OPENAI_API_KEY and EMBEDDING_MODEL)\n",
        "chat_enabled = bool(OPENAI_API_KEY and CHAT_MODEL)\n",
        "llm_enabled = embeddings_enabled or chat_enabled\n",
        "\n",
        "print(\"embeddings_enabled:\", embeddings_enabled)\n",
        "print(\"chat_enabled:\", chat_enabled)\n",
        "print(\"CHAT_MODEL:\", CHAT_MODEL)\n",
        "print(\"EMBEDDING_MODEL:\", EMBEDDING_MODEL)\n",
        "print(\"OPENAI_BASE_URL:\", OPENAI_BASE_URL or \"(default)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e19964",
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg.data_dir.mkdir(parents=True, exist_ok=True)\n",
        "cfg.indexes_dir.mkdir(parents=True, exist_ok=True)\n",
        "cfg.artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
        "cfg.pages_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"project_dir:\", cfg.project_dir)\n",
        "print(\"data_dir:\", cfg.data_dir, \"exists:\", cfg.data_dir.exists())\n",
        "print(\"pages_dir:\", cfg.pages_dir, \"exists:\", cfg.pages_dir.exists())\n",
        "print(\"indexes_dir:\", cfg.indexes_dir, \"exists:\", cfg.indexes_dir.exists())\n",
        "print(\"artifacts_dir:\", cfg.artifacts_dir, \"exists:\", cfg.artifacts_dir.exists())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from src.chunking_pages import split_text_by_page_markers, write_pages\n",
        "\n",
        "page_files = sorted(cfg.pages_dir.glob(\"page_*.txt\"))\n",
        "\n",
        "if page_files and not cfg.rebuild_pages:\n",
        "    print(f\"Pages already exist: {len(page_files)} files. Set cfg.rebuild_pages=True to rebuild.\")\n",
        "else:\n",
        "    if cfg.rebuild_pages and page_files:\n",
        "        for fp in page_files:\n",
        "            fp.unlink()\n",
        "    if cfg.book_txt_path.exists():\n",
        "        book_path = cfg.book_txt_path\n",
        "    elif cfg.book_md_path.exists():\n",
        "        book_path = cfg.book_md_path\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Book not found: expected {cfg.book_txt_path} or {cfg.book_md_path}.\"\n",
        "        )\n",
        "    text = book_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    pages = split_text_by_page_markers(text)\n",
        "    write_pages(pages, cfg.pages_dir)\n",
        "    print(f\"Written {len(pages)} pages to {cfg.pages_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c62c58b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from src.chunking_pages import load_page_chunks\n",
        "\n",
        "chunks = load_page_chunks(cfg.book_id, cfg.pages_dir)\n",
        "chunks_df = pd.DataFrame([c.__dict__ for c in chunks])\n",
        "pages_df = chunks_df[[\"page\", \"text\"]].copy()\n",
        "\n",
        "chunks_df.head(), len(chunks_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ea968d",
      "metadata": {},
      "outputs": [],
      "source": [
        "pages_csv = cfg.pages_csv_path\n",
        "pages_df.to_csv(pages_csv, index=False)\n",
        "print(\"saved:\", pages_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cd135f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks = chunks_df.to_dict(orient=\"records\")\n",
        "chunks_df.head(), len(chunks_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db281a2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.retrievers.bm25 import build_bm25_index, save_bm25\n",
        "\n",
        "bm25_index = build_bm25_index(chunks)\n",
        "bm25_path = cfg.bm25_index_path\n",
        "save_bm25(bm25_index, bm25_path)\n",
        "print(\"saved:\", bm25_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3eb7fd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.embeddings import EmbeddingConfig\n",
        "from src.retrievers.vector_numpy import build_vector_index, save_vector_index\n",
        "\n",
        "if not embeddings_enabled:\n",
        "    print(\"Vector index skipped: embeddings_enabled=False (no API key / embedding model).\")\n",
        "else:\n",
        "    emb_cfg = EmbeddingConfig(\n",
        "        api_key=OPENAI_API_KEY,\n",
        "        base_url=OPENAI_BASE_URL or None,\n",
        "        model=EMBEDDING_MODEL,\n",
        "        batch_size=cfg.embed_batch_size,\n",
        "    )\n",
        "\n",
        "    vector_index = build_vector_index(chunks, emb_cfg)\n",
        "\n",
        "    vec_emb_path = cfg.vector_emb_path\n",
        "    vec_meta_path = cfg.vector_meta_path\n",
        "    save_vector_index(vector_index, vec_emb_path, vec_meta_path)\n",
        "    print(\"saved:\", vec_emb_path)\n",
        "    print(\"saved:\", vec_meta_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e4e786d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_hits(hits, max_chars: int) -> None:\n",
        "    for h in hits:\n",
        "        snippet = \" \".join((h.get(\"text\") or \"\").split())\n",
        "        if len(snippet) > max_chars:\n",
        "            snippet = snippet[:max_chars].rstrip() + \"...\"\n",
        "        print(f\"- page={h['page']} score={h['score']:.4f} | {snippet}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0819d523",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from src.retrieval_pipeline import retrieve, build_context\n",
        "from src.eval import evaluate_questions\n",
        "\n",
        "# bm25_index уже построен ранее\n",
        "# vector_index и emb_cfg существуют только если embeddings_enabled=True\n",
        "\n",
        "def run_retrieve(query: str, mode: str):\n",
        "    return retrieve(\n",
        "        query=query,\n",
        "        mode=mode,\n",
        "        top_k=cfg.top_k,\n",
        "        bm25=bm25_index,\n",
        "        vector=(vector_index if embeddings_enabled else None),\n",
        "        emb_cfg=(emb_cfg if embeddings_enabled else None),\n",
        "        vector_top_k=cfg.vector_top_k,\n",
        "        rrf_k=cfg.rrf_k,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "demo_modes = [\"bm25\"]\n",
        "if embeddings_enabled:\n",
        "    demo_modes += [\"vector\", \"hybrid\"]\n",
        "\n",
        "for mode in demo_modes:\n",
        "    print(f\"\\nMODE: {mode}\")\n",
        "    for q in cfg.demo_queries:\n",
        "        hits = run_retrieve(q, mode)\n",
        "        print(f\"\\nQUERY: {q}\")\n",
        "        print_hits(hits, cfg.demo_snippet_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from src.cost import Pricing, count_tokens, print_cost\n",
        "\n",
        "def _f(x: str, default: float = 0.0) -> float:\n",
        "    try:\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "pricing = Pricing(\n",
        "    embed_price_per_1k_usd=_f(os.getenv(\"EMBED_PRICE_PER_1K_USD\", \"0\")),\n",
        "    chat_in_price_per_1k_usd=_f(os.getenv(\"CHAT_IN_PRICE_PER_1K_USD\", \"0\")),\n",
        "    chat_out_price_per_1k_usd=_f(os.getenv(\"CHAT_OUT_PRICE_PER_1K_USD\", \"0\")),\n",
        ")\n",
        "\n",
        "embedding_tokens_total = None\n",
        "if EMBEDDING_MODEL:\n",
        "    embedding_tokens_total = sum(count_tokens(t, EMBEDDING_MODEL) for t in chunks_df[\"text\"].tolist())\n",
        "\n",
        "if embedding_tokens_total is None:\n",
        "    print(\"Embedding cost skipped: EMBEDDING_MODEL is empty.\")\n",
        "else:\n",
        "    print_cost(embedding_tokens=embedding_tokens_total, pricing=pricing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20009a25",
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "Ты — вопрос-ответный ассистент по одной книге.\n",
        "Правила:\n",
        "1) Отвечай ТОЛЬКО на основе предоставленного КОНТЕКСТА (выдержки со страниц).\n",
        "2) Если в контексте нет ответа — скажи: \"В предоставленном контексте ответа нет\".\n",
        "3) Всегда указывай ссылки на страницы в формате \"стр. N\".\n",
        "4) Не выдумывай факты, определения, команды и численные значения.\n",
        "Тон: нейтральный, технический, краткий.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ask RAG\n",
        "\n",
        "Ниже — пример 2–3 вопросов и ответы системы.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d4e5f85",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.retrieval_pipeline import build_context\n",
        "from src.llm import LLMConfig, generate_answer\n",
        "\n",
        "ask_mode = cfg.retrieval_mode\n",
        "if ask_mode in (\"vector\", \"hybrid\") and not embeddings_enabled:\n",
        "    print(f\"Ask RAG mode '{ask_mode}' skipped: embeddings_enabled=False. Falling back to bm25.\")\n",
        "    ask_mode = \"bm25\"\n",
        "\n",
        "rag_contexts = []\n",
        "rag_usages = []\n",
        "\n",
        "llm_cfg = None\n",
        "if chat_enabled:\n",
        "    llm_cfg = LLMConfig(\n",
        "        api_key=OPENAI_API_KEY,\n",
        "        base_url=OPENAI_BASE_URL or None,\n",
        "        model=CHAT_MODEL,\n",
        "        temperature=cfg.llm_temperature,\n",
        "        max_tokens=cfg.llm_max_tokens,\n",
        "    )\n",
        "\n",
        "for question in cfg.ask_questions:\n",
        "    print(f\"\\nQUESTION: {question}\")\n",
        "    hits = run_retrieve(question, ask_mode)\n",
        "    print_hits(hits, cfg.demo_snippet_chars)\n",
        "    context = build_context(hits, max_chars=cfg.max_context_chars)\n",
        "    rag_contexts.append(context)\n",
        "\n",
        "    if not chat_enabled:\n",
        "        print(\"LLM generation skipped: chat_enabled=False (no API key / chat model).\")\n",
        "        rag_usages.append({})\n",
        "        continue\n",
        "\n",
        "    answer, usage = generate_answer(\n",
        "        question=question,\n",
        "        context=context,\n",
        "        system_prompt=SYSTEM_PROMPT.strip(),\n",
        "        cfg=llm_cfg,\n",
        "    )\n",
        "    rag_usages.append(usage if isinstance(usage, dict) else {})\n",
        "    print(\"\\nANSWER:\\n\", answer)\n",
        "    if usage:\n",
        "        print(\"\\nUSAGE:\\n\", usage)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccbec481",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_tokens_total = None\n",
        "completion_tokens_total = None\n",
        "\n",
        "usage_available = False\n",
        "if isinstance(rag_usages, list):\n",
        "    prompt_sum = 0\n",
        "    completion_sum = 0\n",
        "    for usage in rag_usages:\n",
        "        if isinstance(usage, dict) and \"prompt_tokens\" in usage and \"completion_tokens\" in usage:\n",
        "            usage_available = True\n",
        "            prompt_sum += int(usage.get(\"prompt_tokens\", 0))\n",
        "            completion_sum += int(usage.get(\"completion_tokens\", 0))\n",
        "    if usage_available:\n",
        "        prompt_tokens_total = prompt_sum\n",
        "        completion_tokens_total = completion_sum\n",
        "\n",
        "if not usage_available and CHAT_MODEL:\n",
        "    prompt_tokens_total = sum(\n",
        "        count_tokens(\n",
        "            SYSTEM_PROMPT.strip() + \"\\n\" + ctx + \"\\n\" + q,\n",
        "            CHAT_MODEL,\n",
        "        )\n",
        "        for q, ctx in zip(cfg.ask_questions, rag_contexts)\n",
        "    )\n",
        "    completion_tokens_total = 0\n",
        "\n",
        "if prompt_tokens_total is None or completion_tokens_total is None:\n",
        "    print(\"Chat cost skipped: CHAT_MODEL is empty or Ask RAG was not run.\")\n",
        "    print_cost(embedding_tokens=embedding_tokens_total, pricing=pricing)\n",
        "else:\n",
        "    print_cost(\n",
        "        embedding_tokens=embedding_tokens_total,\n",
        "        prompt_tokens=prompt_tokens_total,\n",
        "        completion_tokens=completion_tokens_total,\n",
        "        pricing=pricing,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13cd0391",
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = json.load(open(cfg.eval_questions_path, \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "page_set = set(int(p) for p in chunks_df[\"page\"].tolist())\n",
        "missing_pages = sorted({int(q[\"gold_page\"]) for q in questions if int(q[\"gold_page\"]) not in page_set})\n",
        "if missing_pages:\n",
        "    raise ValueError(f\"gold_page not found in parsed pages: {missing_pages[:10]}\")\n",
        "\n",
        "modes = [\"bm25\"]\n",
        "if embeddings_enabled:\n",
        "    modes += [\"vector\", \"hybrid\"]\n",
        "\n",
        "rows = evaluate_questions(\n",
        "    questions=questions,\n",
        "    run_retrieve=run_retrieve,\n",
        "    modes=modes,\n",
        "    ks=list(cfg.eval_ks),\n",
        ")\n",
        "\n",
        "eval_df = pd.DataFrame(rows).sort_values([\"mode\", \"k\"])\n",
        "eval_df\n",
        "\n",
        "out_path = cfg.eval_out_path\n",
        "eval_df.to_csv(out_path, index=False)\n",
        "print(\"saved:\", out_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15950712",
      "metadata": {},
      "source": [
        "## Краткий вывод по таблице метрик (≤150 слов)\n",
        "\n",
        "В таблице видно, что увеличение k с 3 до 5 повышает recall для всех режимов,\n",
        "а MRR отражает качество ранжирования на верхних позициях. BM25 дает\n",
        "устойчивый базовый уровень без зависимости от внешних ключей. При наличии\n",
        "эмбеддингов векторный и гибридный режимы обычно дают более высокий recall@5\n",
        "и MRR@5, что особенно полезно для семантических формулировок вопросов.\n",
        "Гибрид сочетает точность лексического поиска и семантику, поэтому его удобно\n",
        "использовать как основной режим.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb362fdc",
      "metadata": {},
      "source": [
        "## Общий вывод (≤200 слов)\n",
        "\n",
        "_Укажи преимущества и 2 улучшения._"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}