{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65fc778",
   "metadata": {},
   "source": [
    "# RAG baseline notebook\n",
    "\n",
    "Краткий запуск:\n",
    "1) Создай venv и установи зависимости из requirements.txt\n",
    "2) Скопируй .env.example -> .env и заполни ключи/тарифы при необходимости\n",
    "3) Restart & Run All\n",
    "\n",
    "Настройки:\n",
    "- Меняются в ячейке Config (book_id, top_k, retrieval_mode, флаги rebuild_*)\n",
    "- Модели/ключи и тарифы берутся из .env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe417412",
   "metadata": {},
   "source": [
    "## Обоснования выбора\n",
    "\n",
    "- Провайдер и модели: использован OpenAI-compatible SDK, чтобы при необходимости менять провайдера через `OPENAI_BASE_URL` без правки кода; модели задаются в `.env`, так как это быстрее всего для подбора баланса качества и стоимости.\n",
    "- Векторный движок: in-memory numpy выбран для прототипа — минимальные зависимости, прозрачная математика и быстрый старт; альтернативы для продакшена: FAISS, Qdrant, Milvus, Weaviate, pgvector.\n",
    "- Page-wise splitting: 1 страница = 1 чанк соответствует ТЗ и метрикам (gold=page), упрощает ссылки на источники.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7f2fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHONPATH root: /home/jebonach/Documents/vs-code/python/RAG_Ass\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "PROJECT_DIR = Path(\"..\").resolve()\n",
    "ENV_PATH = PROJECT_DIR / \".env\"\n",
    "if ENV_PATH.exists():\n",
    "    load_dotenv(ENV_PATH)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    seed: int = int(os.getenv(\"SEED\", \"42\"))\n",
    "\n",
    "    # paths\n",
    "    project_dir: Path = PROJECT_DIR\n",
    "    data_dir: Path = PROJECT_DIR / \"data\"\n",
    "    indexes_dir: Path = PROJECT_DIR / \"indexes\"\n",
    "    artifacts_dir: Path = PROJECT_DIR / \"artifacts\"\n",
    "\n",
    "    # book/page corpus\n",
    "    book_id: str = os.getenv(\"BOOK_ID\", \"devops_handbook\")\n",
    "    rebuild_pages: bool = os.getenv(\"REBUILD_PAGES\", \"true\").lower() == \"true\"\n",
    "    rebuild_indexes: bool = os.getenv(\"REBUILD_INDEXES\", \"true\").lower() == \"true\"\n",
    "\n",
    "    # vector backend\n",
    "    vector_backend: str = os.getenv(\"VECTOR_BACKEND\", \"numpy\").strip().lower()\n",
    "    qdrant_url: str = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\").strip()\n",
    "    qdrant_collection: str = os.getenv(\"QDRANT_COLLECTION\", \"rag\").strip()\n",
    "    rebuild_qdrant: bool = os.getenv(\"REBUILD_QDRANT\", \"false\").lower() == \"true\"\n",
    "    recreate_qdrant_collection: bool = os.getenv(\"RECREATE_QDRANT_COLLECTION\", \"false\").lower() == \"true\"\n",
    "\n",
    "    # retrieval params\n",
    "    retrieval_mode: str = os.getenv(\"RETRIEVAL_MODE\", \"hybrid\")  # bm25 | vector | hybrid\n",
    "    top_k: int = int(os.getenv(\"TOP_K\", \"5\"))\n",
    "    vector_top_k: int = int(os.getenv(\"VECTOR_TOP_K\", \"5\"))\n",
    "    dense_top_k_subchunks: int = int(os.getenv(\"DENSE_TOP_K_SUBCHUNKS\", \"50\"))\n",
    "    subchunk_max_chars: int = int(os.getenv(\"SUBCHUNK_MAX_CHARS\", \"2200\"))\n",
    "    subchunk_overlap: int = int(os.getenv(\"SUBCHUNK_OVERLAP\", \"200\"))\n",
    "    rrf_k: int = int(os.getenv(\"RRF_K\", \"60\"))\n",
    "    max_context_chars: int = int(os.getenv(\"MAX_CONTEXT_CHARS\", \"6000\"))\n",
    "\n",
    "    # embeddings / LLM\n",
    "    embed_batch_size: int = int(os.getenv(\"EMBED_BATCH_SIZE\", \"64\"))\n",
    "    embedding_model: str = os.getenv(\"EMBEDDING_MODEL\", \"\")\n",
    "    chat_model: str = os.getenv(\"CHAT_MODEL\", \"\")\n",
    "    openai_base_url: str = os.getenv(\"OPENAI_BASE_URL\", \"\").strip()\n",
    "    llm_temperature: float = float(os.getenv(\"LLM_TEMPERATURE\", \"0.0\"))\n",
    "    llm_max_tokens: int = int(os.getenv(\"LLM_MAX_TOKENS\", \"600\"))\n",
    "\n",
    "    # evaluation\n",
    "    eval_ks: tuple[int, ...] = (3, 5)\n",
    "\n",
    "    # demo\n",
    "    demo_queries: tuple[str, ...] = (\n",
    "        \"что такое RAG\",\n",
    "        \"индексация\",\n",
    "        \"модель\",\n",
    "    )\n",
    "    ask_questions: tuple[str, ...] = (\n",
    "        \"Как в книге формулируется цель DevOps и почему она важна?\",\n",
    "        \"Какие три пути (The Three Ways) описывает автор и в чем их смысл?\",\n",
    "        \"Что такое value stream mapping и для чего он используется?\",\n",
    "    )\n",
    "    demo_snippet_chars: int = int(os.getenv(\"DEMO_SNIPPET_CHARS\", \"180\"))\n",
    "\n",
    "    @property\n",
    "    def book_dir(self) -> Path:\n",
    "        return self.data_dir / \"books\" / self.book_id\n",
    "\n",
    "    @property\n",
    "    def pages_dir(self) -> Path:\n",
    "        return self.book_dir / \"pages\"\n",
    "\n",
    "    @property\n",
    "    def book_txt_path(self) -> Path:\n",
    "        return self.book_dir / \"book.txt\"\n",
    "\n",
    "    @property\n",
    "    def book_md_path(self) -> Path:\n",
    "        return self.book_dir / \"book.md\"\n",
    "\n",
    "    @property\n",
    "    def pages_csv_path(self) -> Path:\n",
    "        return self.artifacts_dir / \"pages.csv\"\n",
    "\n",
    "    @property\n",
    "    def bm25_index_path(self) -> Path:\n",
    "        return self.indexes_dir / \"bm25.pkl\"\n",
    "\n",
    "    @property\n",
    "    def vector_emb_path(self) -> Path:\n",
    "        return self.indexes_dir / \"vector_embeddings.npy\"\n",
    "\n",
    "    @property\n",
    "    def vector_meta_path(self) -> Path:\n",
    "        return self.indexes_dir / \"vector_meta.json\"\n",
    "\n",
    "    @property\n",
    "    def qdrant_meta_path(self) -> Path:\n",
    "        return self.indexes_dir / f\"qdrant_{self.book_id}_meta.json\"\n",
    "\n",
    "    @property\n",
    "    def eval_questions_path(self) -> Path:\n",
    "        return self.project_dir / \"eval\" / \"questions.json\"\n",
    "\n",
    "    @property\n",
    "    def eval_out_path(self) -> Path:\n",
    "        return self.artifacts_dir / \"retrieval_eval.csv\"\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "cfg\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "\n",
    "# Если Jupyter запустился из notebooks/, поднимемся на уровень выше\n",
    "if not (ROOT / \"src\").exists() and (ROOT.parent / \"src\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "assert (ROOT / \"src\").exists(), f\"Не найден src/ в {ROOT} или {ROOT.parent}\"\n",
    "\n",
    "sys.path.insert(0, str(ROOT))\n",
    "print(\"PYTHONPATH root:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd465e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_enabled: True\n",
      "chat_enabled: True\n",
      "CHAT_MODEL: qwen3:8b\n",
      "EMBEDDING_MODEL: nomic-embed-text\n",
      "OPENAI_BASE_URL: http://localhost:11434/v1/\n",
      "VECTOR_BACKEND: numpy\n",
      "QDRANT_URL: http://localhost:6333\n",
      "QDRANT_COLLECTION: rag\n",
      "SUBCHUNK_MAX_CHARS: 2200\n",
      "SUBCHUNK_OVERLAP: 200\n",
      "DENSE_TOP_K_SUBCHUNKS: 50\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
    "OPENAI_BASE_URL = cfg.openai_base_url\n",
    "CHAT_MODEL = cfg.chat_model\n",
    "EMBEDDING_MODEL = cfg.embedding_model\n",
    "\n",
    "VECTOR_BACKEND = (cfg.vector_backend or \"numpy\").lower()\n",
    "QDRANT_URL = cfg.qdrant_url\n",
    "QDRANT_COLLECTION = cfg.qdrant_collection\n",
    "\n",
    "embeddings_enabled = bool(OPENAI_API_KEY and EMBEDDING_MODEL)\n",
    "chat_enabled = bool(OPENAI_API_KEY and CHAT_MODEL)\n",
    "llm_enabled = embeddings_enabled or chat_enabled\n",
    "\n",
    "print(\"embeddings_enabled:\", embeddings_enabled)\n",
    "print(\"chat_enabled:\", chat_enabled)\n",
    "print(\"CHAT_MODEL:\", CHAT_MODEL)\n",
    "print(\"EMBEDDING_MODEL:\", EMBEDDING_MODEL)\n",
    "print(\"OPENAI_BASE_URL:\", OPENAI_BASE_URL or \"(default)\")\n",
    "print(\"VECTOR_BACKEND:\", VECTOR_BACKEND)\n",
    "print(\"QDRANT_URL:\", QDRANT_URL or \"(not set)\")\n",
    "print(\"QDRANT_COLLECTION:\", QDRANT_COLLECTION)\n",
    "print(\"SUBCHUNK_MAX_CHARS:\", cfg.subchunk_max_chars)\n",
    "print(\"SUBCHUNK_OVERLAP:\", cfg.subchunk_overlap)\n",
    "print(\"DENSE_TOP_K_SUBCHUNKS:\", cfg.dense_top_k_subchunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1bf0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_dir: /home/jebonach/Documents/vs-code/python/RAG_Ass\n",
      "data_dir: /home/jebonach/Documents/vs-code/python/RAG_Ass/data exists: True\n",
      "pages_dir: /home/jebonach/Documents/vs-code/python/RAG_Ass/data/books/devops_handbook/pages exists: True\n",
      "indexes_dir: /home/jebonach/Documents/vs-code/python/RAG_Ass/indexes exists: True\n",
      "artifacts_dir: /home/jebonach/Documents/vs-code/python/RAG_Ass/artifacts exists: True\n"
     ]
    }
   ],
   "source": [
    "cfg.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.indexes_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.pages_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"project_dir:\", cfg.project_dir)\n",
    "print(\"data_dir:\", cfg.data_dir, \"exists:\", cfg.data_dir.exists())\n",
    "print(\"pages_dir:\", cfg.pages_dir, \"exists:\", cfg.pages_dir.exists())\n",
    "print(\"indexes_dir:\", cfg.indexes_dir, \"exists:\", cfg.indexes_dir.exists())\n",
    "print(\"artifacts_dir:\", cfg.artifacts_dir, \"exists:\", cfg.artifacts_dir.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e44674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/jebonach/Documents/vs-code/python/RAG_Ass/notebooks\n",
      "Python: /home/jebonach/Documents/vs-code/python/RAG_Ass/.venv/bin/python3\n",
      "Project root guess: /home/jebonach/Documents/vs-code/python/RAG_Ass/notebooks\n",
      "REPO_ROOT: /home/jebonach/Documents/vs-code/python/RAG_Ass\n",
      "src exists: True\n",
      "cfg.book_txt_path: /home/jebonach/Documents/vs-code/python/RAG_Ass/data/books/devops_handbook/book.txt\n",
      "exists: True\n",
      "absolute: /home/jebonach/Documents/vs-code/python/RAG_Ass/data/books/devops_handbook/book.txt\n",
      "cfg.book_dir: /home/jebonach/Documents/vs-code/python/RAG_Ass/data/books/devops_handbook exists: True\n",
      "cfg.pages_dir: /home/jebonach/Documents/vs-code/python/RAG_Ass/data/books/devops_handbook/pages exists: True\n",
      "os.path.exists: True\n",
      "[WARN] Dropping tail after last page marker: 3392 chars\n",
      "pages count: 449\n",
      "min page: 10 max page: 510\n",
      "first 20: [10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "last 20: [490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 509, 510]\n",
      "Written 449 pages to /home/jebonach/Documents/vs-code/python/RAG_Ass/data/books/devops_handbook/pages\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Project root guess:\", Path.cwd().resolve())\n",
    "print(\"REPO_ROOT:\", ROOT)\n",
    "print(\"src exists:\", (ROOT / \"src\").exists())\n",
    "\n",
    "print(\"cfg.book_txt_path:\", cfg.book_txt_path)\n",
    "print(\"exists:\", cfg.book_txt_path.exists())\n",
    "print(\"absolute:\", cfg.book_txt_path.resolve())\n",
    "print(\"cfg.book_dir:\", cfg.book_dir, \"exists:\", cfg.book_dir.exists())\n",
    "print(\"cfg.pages_dir:\", cfg.pages_dir, \"exists:\", cfg.pages_dir.exists())\n",
    "\n",
    "# Проверка напрямую системным вызовом\n",
    "print(\"os.path.exists:\", os.path.exists(str(cfg.book_txt_path)))\n",
    "\n",
    "from src.chunking_pages import split_text_by_page_markers, write_pages\n",
    "\n",
    "page_files = sorted(cfg.pages_dir.glob(\"page_*.txt\"))\n",
    "\n",
    "if page_files and not cfg.rebuild_pages:\n",
    "    print(f\"Pages already exist: {len(page_files)} files. Set cfg.rebuild_pages=True to rebuild.\")\n",
    "else:\n",
    "    if cfg.rebuild_pages and page_files:\n",
    "        for fp in page_files:\n",
    "            fp.unlink()\n",
    "    if cfg.book_txt_path.exists():\n",
    "        book_path = cfg.book_txt_path\n",
    "    elif cfg.book_md_path.exists():\n",
    "        book_path = cfg.book_md_path\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Book not found: expected {cfg.book_txt_path} or {cfg.book_md_path}.\"\n",
    "        )\n",
    "    text = book_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    pages = split_text_by_page_markers(text)\n",
    "    page_numbers = sorted({p for (p, _) in pages})\n",
    "    print(\"pages count:\", len(page_numbers))\n",
    "    print(\"min page:\", page_numbers[0], \"max page:\", page_numbers[-1])\n",
    "    print(\"first 20:\", page_numbers[:20])\n",
    "    print(\"last 20:\", page_numbers[-20:])\n",
    "    write_pages(pages, cfg.pages_dir)\n",
    "    print(f\"Written {len(pages)} pages to {cfg.pages_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ae5fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page text lengths (chars):\n",
      "  min: 270\n",
      "  p50: 1900\n",
      "  p90: 2208\n",
      "  p99: 3256\n",
      "  max: 7657\n",
      "top 10 lengths: [7657, 3327, 3310, 3301, 3289, 3222, 3200, 2993, 2969, 2939]\n",
      "length histogram (chars):\n",
      "  0-499: 3\n",
      "  500-999: 1\n",
      "  1000-1499: 34\n",
      "  1500-1999: 251\n",
      "  2000-2499: 140\n",
      "  2500-2999: 13\n",
      "  3000-3999: 6\n",
      "  4000-4999: 0\n",
      "  5000-7999: 1\n",
      "  8000-11999: 0\n",
      "  12000-19999: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           book_id  page               chunk_id  \\\n",
       " 0  devops_handbook    10  devops_handbook:p0010   \n",
       " 1  devops_handbook    12  devops_handbook:p0012   \n",
       " 2  devops_handbook    13  devops_handbook:p0013   \n",
       " 3  devops_handbook    14  devops_handbook:p0014   \n",
       " 4  devops_handbook    15  devops_handbook:p0015   \n",
       " \n",
       "                                                 text  \\\n",
       " 0  Научный редактор Николай Корытко\\nИздано с раз...   \n",
       " 1  Предисловие к российскому изданию\\nразвертыван...   \n",
       " 2  Введение\\nЭто было в  2006 г., и  мне тогда пр...   \n",
       " 3  «Ага!»\\nудовлетворение я испытал, видя видя от...   \n",
       " 4  Введение\\nПатрик Дюбуа\\nДля меня это была цела...   \n",
       " \n",
       "                                                 path  \n",
       " 0  /home/jebonach/Documents/vs-code/python/RAG_As...  \n",
       " 1  /home/jebonach/Documents/vs-code/python/RAG_As...  \n",
       " 2  /home/jebonach/Documents/vs-code/python/RAG_As...  \n",
       " 3  /home/jebonach/Documents/vs-code/python/RAG_As...  \n",
       " 4  /home/jebonach/Documents/vs-code/python/RAG_As...  ,\n",
       " 449)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.chunking_pages import load_page_chunks\n",
    "\n",
    "chunks = load_page_chunks(cfg.book_id, cfg.pages_dir)\n",
    "chunks_df = pd.DataFrame([c.__dict__ for c in chunks])\n",
    "pages_df = chunks_df[[\"page\", \"text\"]].copy()\n",
    "\n",
    "lengths = [len(str(t)) for t in chunks_df[\"text\"].tolist()]\n",
    "if lengths:\n",
    "    arr = np.array(lengths)\n",
    "    print(\"page text lengths (chars):\")\n",
    "    print(\"  min:\", int(arr.min()))\n",
    "    print(\"  p50:\", int(np.percentile(arr, 50)))\n",
    "    print(\"  p90:\", int(np.percentile(arr, 90)))\n",
    "    print(\"  p99:\", int(np.percentile(arr, 99)))\n",
    "    print(\"  max:\", int(arr.max()))\n",
    "    print(\"top 10 lengths:\", sorted(lengths, reverse=True)[:10])\n",
    "    bins = [0, 500, 1000, 1500, 2000, 2500, 3000, 4000, 5000, 8000, 12000, 20000]\n",
    "    print(\"length histogram (chars):\")\n",
    "    for i in range(len(bins) - 1):\n",
    "        lo = bins[i]\n",
    "        hi = bins[i + 1] - 1\n",
    "        count = int(((arr >= lo) & (arr <= hi)).sum())\n",
    "        print(f\"  {lo}-{hi}: {count}\")\n",
    "\n",
    "chunks_df.head(), len(chunks_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d46b5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/jebonach/Documents/vs-code/python/RAG_Ass/artifacts/pages.csv\n"
     ]
    }
   ],
   "source": [
    "pages_csv = cfg.pages_csv_path\n",
    "pages_df.to_csv(pages_csv, index=False)\n",
    "print(\"saved:\", pages_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d56cfee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(           book_id  page               chunk_id  \\\n",
       " 0  devops_handbook    10  devops_handbook:p0010   \n",
       " 1  devops_handbook    12  devops_handbook:p0012   \n",
       " 2  devops_handbook    13  devops_handbook:p0013   \n",
       " 3  devops_handbook    14  devops_handbook:p0014   \n",
       " 4  devops_handbook    15  devops_handbook:p0015   \n",
       " \n",
       "                                                 text  \\\n",
       " 0  Научный редактор Николай Корытко\\nИздано с раз...   \n",
       " 1  Предисловие к российскому изданию\\nразвертыван...   \n",
       " 2  Введение\\nЭто было в  2006 г., и  мне тогда пр...   \n",
       " 3  «Ага!»\\nудовлетворение я испытал, видя видя от...   \n",
       " 4  Введение\\nПатрик Дюбуа\\nДля меня это была цела...   \n",
       " \n",
       "                                                 path  \n",
       " 0  /home/jebonach/Documents/vs-code/python/RAG_As...  \n",
       " 1  /home/jebonach/Documents/vs-code/python/RAG_As...  \n",
       " 2  /home/jebonach/Documents/vs-code/python/RAG_As...  \n",
       " 3  /home/jebonach/Documents/vs-code/python/RAG_As...  \n",
       " 4  /home/jebonach/Documents/vs-code/python/RAG_As...  ,\n",
       " 449)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunks_df.to_dict(orient=\"records\")\n",
    "chunks_df.head(), len(chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a655f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/jebonach/Documents/vs-code/python/RAG_Ass/indexes/bm25.pkl\n"
     ]
    }
   ],
   "source": [
    "from src.retrievers.bm25 import build_bm25_index, load_bm25, save_bm25\n",
    "\n",
    "bm25_path = cfg.bm25_index_path\n",
    "if bm25_path.exists() and not cfg.rebuild_indexes:\n",
    "    bm25_index = load_bm25(bm25_path)\n",
    "    print(\"loaded:\", bm25_path)\n",
    "else:\n",
    "    bm25_index = build_bm25_index(chunks)\n",
    "    save_bm25(bm25_index, bm25_path)\n",
    "    print(\"saved:\", bm25_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b32db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VECTOR_BACKEND: numpy\n"
     ]
    }
   ],
   "source": [
    "from src.embeddings import EmbeddingConfig\n",
    "\n",
    "VECTOR_BACKEND = (cfg.vector_backend or \"numpy\").lower()\n",
    "if VECTOR_BACKEND not in (\"numpy\", \"qdrant\"):\n",
    "    print(f\"Unknown VECTOR_BACKEND '{VECTOR_BACKEND}', falling back to numpy.\")\n",
    "    VECTOR_BACKEND = \"numpy\"\n",
    "print(\"VECTOR_BACKEND:\", VECTOR_BACKEND)\n",
    "\n",
    "vector_enabled = False\n",
    "vector_index = None\n",
    "emb_cfg = None\n",
    "\n",
    "if not embeddings_enabled:\n",
    "    print(\"Vector index skipped: embeddings_enabled=False (no API key / embedding model).\")\n",
    "else:\n",
    "    emb_cfg = EmbeddingConfig(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        base_url=OPENAI_BASE_URL or None,\n",
    "        model=EMBEDDING_MODEL,\n",
    "        batch_size=cfg.embed_batch_size,\n",
    "    )\n",
    "\n",
    "    if VECTOR_BACKEND == \"qdrant\":\n",
    "        from src.retrievers.vector_qdrant import build_qdrant_index, load_qdrant_meta, QdrantIndex\n",
    "        from src.vectorstores.qdrant_store import QdrantVectorStore, get_client\n",
    "\n",
    "        qdrant_url = cfg.qdrant_url\n",
    "        base_collection = cfg.qdrant_collection\n",
    "        meta_path = cfg.qdrant_meta_path\n",
    "        rebuild_qdrant = cfg.rebuild_qdrant or cfg.rebuild_indexes\n",
    "        recreate_qdrant_collection = cfg.recreate_qdrant_collection\n",
    "\n",
    "        print(\"QDRANT_URL:\", qdrant_url)\n",
    "        print(\"QDRANT_COLLECTION_BASE:\", base_collection)\n",
    "        print(\"qdrant_meta_path:\", meta_path)\n",
    "\n",
    "        if not qdrant_url:\n",
    "            print(\"Qdrant URL is empty, skipping vector/hybrid.\")\n",
    "        else:\n",
    "            try:\n",
    "                client = get_client(qdrant_url)\n",
    "                client.get_collections()\n",
    "                qdrant_ready = True\n",
    "            except Exception as exc:\n",
    "                print(\"Qdrant unavailable, skipping vector/hybrid:\", exc)\n",
    "                qdrant_ready = False\n",
    "\n",
    "            if qdrant_ready:\n",
    "                page_text_by_page = {int(ch[\"page\"]): str(ch.get(\"text\") or \"\") for ch in chunks}\n",
    "                page_chunk_id_by_page = {int(ch[\"page\"]): str(ch.get(\"chunk_id\", f\"p{ch['page']}\")) for ch in chunks}\n",
    "\n",
    "                if meta_path.exists() and not rebuild_qdrant and not recreate_qdrant_collection:\n",
    "                    meta = load_qdrant_meta(meta_path)\n",
    "                    collection_name = meta.get(\"collection\")\n",
    "                    print(\"loaded:\", meta_path)\n",
    "                    print(\"QDRANT_COLLECTION:\", collection_name)\n",
    "\n",
    "                    if meta.get(\"embedding_model\") != EMBEDDING_MODEL:\n",
    "                        print(\"Qdrant meta embedding_model mismatch; rebuilding.\")\n",
    "                        rebuild_qdrant = True\n",
    "\n",
    "                    params = meta.get(\"subchunk_params\", {})\n",
    "                    if params.get(\"max_chars\") != cfg.subchunk_max_chars or params.get(\"overlap\") != cfg.subchunk_overlap:\n",
    "                        print(\"Qdrant meta subchunk params mismatch; rebuilding.\")\n",
    "                        rebuild_qdrant = True\n",
    "\n",
    "                    if not rebuild_qdrant and collection_name:\n",
    "                        if not client.collection_exists(collection_name):\n",
    "                            print(\"Qdrant collection missing; rebuilding.\")\n",
    "                            rebuild_qdrant = True\n",
    "\n",
    "                    if not rebuild_qdrant and collection_name:\n",
    "                        print(\"Using existing Qdrant collection.\")\n",
    "                        store = QdrantVectorStore(client=client, collection=collection_name)\n",
    "                        vector_index = QdrantIndex(\n",
    "                            store=store,\n",
    "                            emb_cfg=emb_cfg,\n",
    "                            page_text_by_page=page_text_by_page,\n",
    "                            page_chunk_id_by_page=page_chunk_id_by_page,\n",
    "                            top_k_subchunks=cfg.dense_top_k_subchunks,\n",
    "                            subchunk_max_chars=cfg.subchunk_max_chars,\n",
    "                            subchunk_overlap=cfg.subchunk_overlap,\n",
    "                            snippet_chars=cfg.demo_snippet_chars,\n",
    "                        )\n",
    "                        vector_enabled = True\n",
    "\n",
    "                if not vector_enabled:\n",
    "                    try:\n",
    "                        print(\"Building Qdrant index...\")\n",
    "                        vector_index, meta = build_qdrant_index(\n",
    "                            client=client,\n",
    "                            base_collection=base_collection,\n",
    "                            book_id=cfg.book_id,\n",
    "                            pages=chunks,\n",
    "                            emb_cfg=emb_cfg,\n",
    "                            subchunk_max_chars=cfg.subchunk_max_chars,\n",
    "                            subchunk_overlap=cfg.subchunk_overlap,\n",
    "                            recreate_collection=recreate_qdrant_collection,\n",
    "                            batch_size=cfg.embed_batch_size,\n",
    "                            snippet_chars=cfg.demo_snippet_chars,\n",
    "                            top_k_subchunks=cfg.dense_top_k_subchunks,\n",
    "                            meta_path=meta_path,\n",
    "                        )\n",
    "                        print(\"QDRANT_COLLECTION:\", meta.get(\"collection\"))\n",
    "                        print(\"saved:\", meta_path)\n",
    "                        vector_enabled = True\n",
    "                    except Exception as exc:\n",
    "                        print(\"Qdrant index build failed, skipping vector/hybrid:\", exc)\n",
    "                        vector_enabled = False\n",
    "\n",
    "    else:\n",
    "        from src.retrievers.vector_numpy import build_vector_index, load_vector_index, save_vector_index\n",
    "\n",
    "        vec_emb_path = cfg.vector_emb_path\n",
    "        vec_meta_path = cfg.vector_meta_path\n",
    "\n",
    "        if vec_emb_path.exists() and vec_meta_path.exists() and not cfg.rebuild_indexes:\n",
    "            vector_index = load_vector_index(vec_emb_path, vec_meta_path)\n",
    "            print(\"loaded:\", vec_emb_path)\n",
    "            print(\"loaded:\", vec_meta_path)\n",
    "        else:\n",
    "            vector_index = build_vector_index(\n",
    "                chunks,\n",
    "                emb_cfg,\n",
    "                subchunk_max_chars=cfg.subchunk_max_chars,\n",
    "                subchunk_overlap=cfg.subchunk_overlap,\n",
    "                snippet_chars=cfg.demo_snippet_chars,\n",
    "            )\n",
    "            save_vector_index(vector_index, vec_emb_path, vec_meta_path)\n",
    "            print(\"saved:\", vec_emb_path)\n",
    "            print(\"saved:\", vec_meta_path)\n",
    "\n",
    "        vector_enabled = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hits(hits, max_chars: int) -> None:\n",
    "    for h in hits:\n",
    "        snippet = \" \".join((h.get(\"text\") or \"\").split())\n",
    "        if len(snippet) > max_chars:\n",
    "            snippet = snippet[:max_chars].rstrip() + \"...\"\n",
    "        print(f\"- page={h['page']} score={h['score']:.4f} | {snippet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from src.retrieval_pipeline import retrieve, build_context\n",
    "from src.eval import evaluate_questions\n",
    "\n",
    "# bm25_index already built earlier\n",
    "# vector_index and emb_cfg exist only if vector_enabled=True\n",
    "\n",
    "def run_retrieve(query: str, mode: str):\n",
    "    return retrieve(\n",
    "        query=query,\n",
    "        mode=mode,\n",
    "        top_k=cfg.top_k,\n",
    "        bm25=bm25_index,\n",
    "        vector=(vector_index if vector_enabled else None),\n",
    "        emb_cfg=(emb_cfg if vector_enabled else None),\n",
    "        vector_top_k=cfg.vector_top_k,\n",
    "        vector_subchunk_k=cfg.dense_top_k_subchunks,\n",
    "        rrf_k=cfg.rrf_k,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_modes = [\"bm25\"]\n",
    "if vector_enabled:\n",
    "    demo_modes += [\"vector\", \"hybrid\"]\n",
    "\n",
    "for mode in demo_modes:\n",
    "    print(f\"\\nMODE: {mode}\")\n",
    "    for q in cfg.demo_queries:\n",
    "        hits = run_retrieve(q, mode)\n",
    "        print(f\"\\nQUERY: {q}\")\n",
    "        print_hits(hits, cfg.demo_snippet_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94efefed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.cost import Pricing, count_tokens, print_cost\n",
    "\n",
    "def _f(x: str, default: float = 0.0) -> float:\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "pricing = Pricing(\n",
    "    embed_price_per_1k_usd=_f(os.getenv(\"EMBED_PRICE_PER_1K_USD\", \"0\")),\n",
    "    chat_in_price_per_1k_usd=_f(os.getenv(\"CHAT_IN_PRICE_PER_1K_USD\", \"0\")),\n",
    "    chat_out_price_per_1k_usd=_f(os.getenv(\"CHAT_OUT_PRICE_PER_1K_USD\", \"0\")),\n",
    ")\n",
    "\n",
    "embedding_tokens_total = None\n",
    "if EMBEDDING_MODEL:\n",
    "    embedding_tokens_total = sum(count_tokens(t, EMBEDDING_MODEL) for t in chunks_df[\"text\"].tolist())\n",
    "\n",
    "if embedding_tokens_total is None:\n",
    "    print(\"Embedding cost skipped: EMBEDDING_MODEL is empty.\")\n",
    "else:\n",
    "    print_cost(embedding_tokens=embedding_tokens_total, pricing=pricing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Ты — вопрос-ответный ассистент по одной книге.\n",
    "Правила:\n",
    "1) Отвечай ТОЛЬКО на основе предоставленного КОНТЕКСТА (выдержки со страниц).\n",
    "2) Если в контексте нет ответа — скажи: \"В предоставленном контексте ответа нет\".\n",
    "3) Всегда указывай ссылки на страницы в формате \"стр. N\".\n",
    "4) Не выдумывай факты, определения, команды и численные значения.\n",
    "Тон: нейтральный, технический, краткий.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c902b8",
   "metadata": {},
   "source": [
    "## Ask RAG\n",
    "\n",
    "Ниже — пример 2-3 вопросов и ответы системы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retrieval_pipeline import build_context\n",
    "from src.llm import LLMConfig, generate_answer\n",
    "\n",
    "ask_mode = cfg.retrieval_mode\n",
    "if ask_mode in (\"vector\", \"hybrid\") and not vector_enabled:\n",
    "    print(f\"Ask RAG mode '{ask_mode}' skipped: vector backend unavailable. Falling back to bm25.\")\n",
    "    ask_mode = \"bm25\"\n",
    "\n",
    "rag_contexts = []\n",
    "rag_usages = []\n",
    "\n",
    "llm_cfg = None\n",
    "if chat_enabled:\n",
    "    llm_cfg = LLMConfig(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        base_url=OPENAI_BASE_URL or None,\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=cfg.llm_temperature,\n",
    "        max_tokens=cfg.llm_max_tokens,\n",
    "    )\n",
    "\n",
    "for question in cfg.ask_questions:\n",
    "    print(f\"\\nQUESTION: {question}\")\n",
    "    hits = run_retrieve(question, ask_mode)\n",
    "    print_hits(hits, cfg.demo_snippet_chars)\n",
    "    context = build_context(hits, max_chars=cfg.max_context_chars)\n",
    "    rag_contexts.append(context)\n",
    "\n",
    "    if not chat_enabled:\n",
    "        print(\"LLM generation skipped: chat_enabled=False (no API key / chat model).\")\n",
    "        rag_usages.append({})\n",
    "        continue\n",
    "\n",
    "    answer, usage = generate_answer(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        system_prompt=SYSTEM_PROMPT.strip(),\n",
    "        cfg=llm_cfg,\n",
    "    )\n",
    "    rag_usages.append(usage if isinstance(usage, dict) else {})\n",
    "    print(\"\\nANSWER:\\n\", answer)\n",
    "    if usage:\n",
    "        print(\"\\nUSAGE:\\n\", usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb39913",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens_total = None\n",
    "completion_tokens_total = None\n",
    "\n",
    "usage_available = False\n",
    "if isinstance(rag_usages, list):\n",
    "    prompt_sum = 0\n",
    "    completion_sum = 0\n",
    "    for usage in rag_usages:\n",
    "        if isinstance(usage, dict) and \"prompt_tokens\" in usage and \"completion_tokens\" in usage:\n",
    "            usage_available = True\n",
    "            prompt_sum += int(usage.get(\"prompt_tokens\", 0))\n",
    "            completion_sum += int(usage.get(\"completion_tokens\", 0))\n",
    "    if usage_available:\n",
    "        prompt_tokens_total = prompt_sum\n",
    "        completion_tokens_total = completion_sum\n",
    "\n",
    "if not usage_available and CHAT_MODEL:\n",
    "    prompt_tokens_total = sum(\n",
    "        count_tokens(\n",
    "            SYSTEM_PROMPT.strip() + \"\\n\" + ctx + \"\\n\" + q,\n",
    "            CHAT_MODEL,\n",
    "        )\n",
    "        for q, ctx in zip(cfg.ask_questions, rag_contexts)\n",
    "    )\n",
    "    completion_tokens_total = 0\n",
    "\n",
    "if prompt_tokens_total is None or completion_tokens_total is None:\n",
    "    print(\"Chat cost skipped: CHAT_MODEL is empty or Ask RAG was not run.\")\n",
    "    print_cost(embedding_tokens=embedding_tokens_total, pricing=pricing)\n",
    "else:\n",
    "    print_cost(\n",
    "        embedding_tokens=embedding_tokens_total,\n",
    "        prompt_tokens=prompt_tokens_total,\n",
    "        completion_tokens=completion_tokens_total,\n",
    "        pricing=pricing,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caaf05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = json.load(open(cfg.eval_questions_path, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "page_set = set(int(p) for p in chunks_df[\"page\"].tolist())\n",
    "missing_pages = sorted({int(q[\"gold_page\"]) for q in questions if int(q[\"gold_page\"]) not in page_set})\n",
    "if missing_pages:\n",
    "    raise ValueError(f\"gold_page not found in parsed pages: {missing_pages[:10]}\")\n",
    "\n",
    "modes = [\"bm25\"]\n",
    "if vector_enabled:\n",
    "    modes += [\"vector\", \"hybrid\"]\n",
    "\n",
    "rows = evaluate_questions(\n",
    "    questions=questions,\n",
    "    run_retrieve=run_retrieve,\n",
    "    modes=modes,\n",
    "    ks=list(cfg.eval_ks),\n",
    ")\n",
    "\n",
    "eval_df = pd.DataFrame(rows).sort_values([\"mode\", \"k\"])\n",
    "eval_df\n",
    "\n",
    "out_path = cfg.eval_out_path\n",
    "eval_df.to_csv(out_path, index=False)\n",
    "print(\"saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b69c96b",
   "metadata": {},
   "source": [
    "## Краткий вывод по таблице метрик (≤150 слов)\n",
    "\n",
    "В таблице видно, что увеличение k с 3 до 5 повышает recall для всех режимов,\n",
    "а MRR отражает качество ранжирования на верхних позициях. BM25 дает\n",
    "устойчивый базовый уровень без зависимости от внешних ключей. При наличии\n",
    "эмбеддингов векторный и гибридный режимы обычно дают более высокий recall@5\n",
    "и MRR@5, что особенно полезно для семантических формулировок вопросов.\n",
    "Гибрид сочетает точность лексического поиска и семантику, поэтому его удобно\n",
    "использовать как основной режим.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071ab98",
   "metadata": {},
   "source": [
    "## Общий вывод (≤200 слов)\n",
    "\n",
    "Система обеспечивает воспроизводимый RAG-пайплайн: строгое постраничное\n",
    "разбиение гарантирует корректные ссылки, единый интерфейс ретривера\n",
    "упрощает сравнение режимов, а гибридный поиск сочетает лексическую точность\n",
    "BM25 и семантику эмбеддингов. Встроенная оценка (recall@k и MRR@k)\n",
    "показывает качество поиска, а прозрачный учет стоимости помогает планировать\n",
    "затраты при масштабировании.\n",
    "\n",
    "Идеи улучшения: (1) добавить re-ranker на кросс-энкодере для более точного\n",
    "упорядочивания top-k; (2) заменить in-memory индекс на постоянное векторное\n",
    "хранилище (например, FAISS или Qdrant) с кэшированием эмбеддингов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47892ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229aa2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d032d102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e46f492b",
   "metadata": {},
   "source": [
    "# RAG baseline notebook\n",
    "\n",
    "Краткий запуск:\n",
    "1) Создай venv и установи зависимости из requirements.txt\n",
    "2) Скопируй .env.example -> .env и заполни ключи/тарифы при необходимости\n",
    "3) Restart & Run All\n",
    "\n",
    "Настройки:\n",
    "- Меняются в ячейке Config (book_id, top_k, retrieval_mode, флаги rebuild_*)\n",
    "- Модели/ключи и тарифы берутся из .env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec26a2c",
   "metadata": {},
   "source": [
    "## Обоснования выбора\n",
    "\n",
    "- Провайдер и модели: использован OpenAI-compatible SDK, чтобы при необходимости менять провайдера через `OPENAI_BASE_URL` без правки кода; модели задаются в `.env`, так как это быстрее всего для подбора баланса качества и стоимости.\n",
    "- Векторный движок: in-memory numpy выбран для прототипа — минимальные зависимости, прозрачная математика и быстрый старт; альтернативы для продакшена: FAISS, Qdrant, Milvus, Weaviate, pgvector.\n",
    "- Page-wise splitting: 1 страница = 1 чанк соответствует ТЗ и метрикам (gold=page), упрощает ссылки на источники.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "PROJECT_DIR = Path(\"..\").resolve()\n",
    "ENV_PATH = PROJECT_DIR / \".env\"\n",
    "if ENV_PATH.exists():\n",
    "    load_dotenv(ENV_PATH)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    seed: int = int(os.getenv(\"SEED\", \"42\"))\n",
    "\n",
    "    # paths\n",
    "    project_dir: Path = PROJECT_DIR\n",
    "    data_dir: Path = PROJECT_DIR / \"data\"\n",
    "    indexes_dir: Path = PROJECT_DIR / \"indexes\"\n",
    "    artifacts_dir: Path = PROJECT_DIR / \"artifacts\"\n",
    "\n",
    "    # book/page corpus\n",
    "    book_id: str = os.getenv(\"BOOK_ID\", \"devops_handbook\")\n",
    "    rebuild_pages: bool = os.getenv(\"REBUILD_PAGES\", \"true\").lower() == \"true\"\n",
    "    rebuild_indexes: bool = os.getenv(\"REBUILD_INDEXES\", \"true\").lower() == \"true\"\n",
    "\n",
    "    # vector backend\n",
    "    vector_backend: str = os.getenv(\"VECTOR_BACKEND\", \"numpy\").strip().lower()\n",
    "    qdrant_url: str = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\").strip()\n",
    "    qdrant_collection: str = os.getenv(\"QDRANT_COLLECTION\", \"rag\").strip()\n",
    "    rebuild_qdrant: bool = os.getenv(\"REBUILD_QDRANT\", \"false\").lower() == \"true\"\n",
    "    recreate_qdrant_collection: bool = os.getenv(\"RECREATE_QDRANT_COLLECTION\", \"false\").lower() == \"true\"\n",
    "\n",
    "    # retrieval params\n",
    "    retrieval_mode: str = os.getenv(\"RETRIEVAL_MODE\", \"hybrid\")  # bm25 | vector | hybrid\n",
    "    top_k: int = int(os.getenv(\"TOP_K\", \"5\"))\n",
    "    vector_top_k: int = int(os.getenv(\"VECTOR_TOP_K\", \"5\"))\n",
    "    dense_top_k_subchunks: int = int(os.getenv(\"DENSE_TOP_K_SUBCHUNKS\", \"50\"))\n",
    "    subchunk_max_chars: int = int(os.getenv(\"SUBCHUNK_MAX_CHARS\", \"2200\"))\n",
    "    subchunk_overlap: int = int(os.getenv(\"SUBCHUNK_OVERLAP\", \"200\"))\n",
    "    rrf_k: int = int(os.getenv(\"RRF_K\", \"60\"))\n",
    "    max_context_chars: int = int(os.getenv(\"MAX_CONTEXT_CHARS\", \"6000\"))\n",
    "\n",
    "    # embeddings / LLM\n",
    "    embed_batch_size: int = int(os.getenv(\"EMBED_BATCH_SIZE\", \"64\"))\n",
    "    embedding_model: str = os.getenv(\"EMBEDDING_MODEL\", \"\")\n",
    "    chat_model: str = os.getenv(\"CHAT_MODEL\", \"\")\n",
    "    openai_base_url: str = os.getenv(\"OPENAI_BASE_URL\", \"\").strip()\n",
    "    llm_temperature: float = float(os.getenv(\"LLM_TEMPERATURE\", \"0.0\"))\n",
    "    llm_max_tokens: int = int(os.getenv(\"LLM_MAX_TOKENS\", \"600\"))\n",
    "\n",
    "    # evaluation\n",
    "    eval_ks: tuple[int, ...] = (3, 5)\n",
    "\n",
    "    # demo\n",
    "    demo_queries: tuple[str, ...] = (\n",
    "        \"что такое RAG\",\n",
    "        \"индексация\",\n",
    "        \"модель\",\n",
    "    )\n",
    "    ask_questions: tuple[str, ...] = (\n",
    "        \"Как в книге формулируется цель DevOps и почему она важна?\",\n",
    "        \"Какие три пути (The Three Ways) описывает автор и в чем их смысл?\",\n",
    "        \"Что такое value stream mapping и для чего он используется?\",\n",
    "    )\n",
    "    demo_snippet_chars: int = int(os.getenv(\"DEMO_SNIPPET_CHARS\", \"180\"))\n",
    "\n",
    "    @property\n",
    "    def book_dir(self) -> Path:\n",
    "        return self.data_dir / \"books\" / self.book_id\n",
    "\n",
    "    @property\n",
    "    def pages_dir(self) -> Path:\n",
    "        return self.book_dir / \"pages\"\n",
    "\n",
    "    @property\n",
    "    def book_txt_path(self) -> Path:\n",
    "        return self.book_dir / \"book.txt\"\n",
    "\n",
    "    @property\n",
    "    def book_md_path(self) -> Path:\n",
    "        return self.book_dir / \"book.md\"\n",
    "\n",
    "    @property\n",
    "    def pages_csv_path(self) -> Path:\n",
    "        return self.artifacts_dir / \"pages.csv\"\n",
    "\n",
    "    @property\n",
    "    def bm25_index_path(self) -> Path:\n",
    "        return self.indexes_dir / \"bm25.pkl\"\n",
    "\n",
    "    @property\n",
    "    def vector_emb_path(self) -> Path:\n",
    "        return self.indexes_dir / \"vector_embeddings.npy\"\n",
    "\n",
    "    @property\n",
    "    def vector_meta_path(self) -> Path:\n",
    "        return self.indexes_dir / \"vector_meta.json\"\n",
    "\n",
    "    @property\n",
    "    def qdrant_meta_path(self) -> Path:\n",
    "        return self.indexes_dir / f\"qdrant_{self.book_id}_meta.json\"\n",
    "\n",
    "    @property\n",
    "    def eval_questions_path(self) -> Path:\n",
    "        return self.project_dir / \"eval\" / \"questions.json\"\n",
    "\n",
    "    @property\n",
    "    def eval_out_path(self) -> Path:\n",
    "        return self.artifacts_dir / \"retrieval_eval.csv\"\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "cfg\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "\n",
    "# Если Jupyter запустился из notebooks/, поднимемся на уровень выше\n",
    "if not (ROOT / \"src\").exists() and (ROOT.parent / \"src\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "assert (ROOT / \"src\").exists(), f\"Не найден src/ в {ROOT} или {ROOT.parent}\"\n",
    "\n",
    "sys.path.insert(0, str(ROOT))\n",
    "print(\"PYTHONPATH root:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4086a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
    "OPENAI_BASE_URL = cfg.openai_base_url\n",
    "CHAT_MODEL = cfg.chat_model\n",
    "EMBEDDING_MODEL = cfg.embedding_model\n",
    "\n",
    "VECTOR_BACKEND = (cfg.vector_backend or \"numpy\").lower()\n",
    "QDRANT_URL = cfg.qdrant_url\n",
    "QDRANT_COLLECTION = cfg.qdrant_collection\n",
    "\n",
    "embeddings_enabled = bool(OPENAI_API_KEY and EMBEDDING_MODEL)\n",
    "chat_enabled = bool(OPENAI_API_KEY and CHAT_MODEL)\n",
    "llm_enabled = embeddings_enabled or chat_enabled\n",
    "\n",
    "print(\"embeddings_enabled:\", embeddings_enabled)\n",
    "print(\"chat_enabled:\", chat_enabled)\n",
    "print(\"CHAT_MODEL:\", CHAT_MODEL)\n",
    "print(\"EMBEDDING_MODEL:\", EMBEDDING_MODEL)\n",
    "print(\"OPENAI_BASE_URL:\", OPENAI_BASE_URL or \"(default)\")\n",
    "print(\"VECTOR_BACKEND:\", VECTOR_BACKEND)\n",
    "print(\"QDRANT_URL:\", QDRANT_URL or \"(not set)\")\n",
    "print(\"QDRANT_COLLECTION:\", QDRANT_COLLECTION)\n",
    "print(\"SUBCHUNK_MAX_CHARS:\", cfg.subchunk_max_chars)\n",
    "print(\"SUBCHUNK_OVERLAP:\", cfg.subchunk_overlap)\n",
    "print(\"DENSE_TOP_K_SUBCHUNKS:\", cfg.dense_top_k_subchunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.indexes_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.pages_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"project_dir:\", cfg.project_dir)\n",
    "print(\"data_dir:\", cfg.data_dir, \"exists:\", cfg.data_dir.exists())\n",
    "print(\"pages_dir:\", cfg.pages_dir, \"exists:\", cfg.pages_dir.exists())\n",
    "print(\"indexes_dir:\", cfg.indexes_dir, \"exists:\", cfg.indexes_dir.exists())\n",
    "print(\"artifacts_dir:\", cfg.artifacts_dir, \"exists:\", cfg.artifacts_dir.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Project root guess:\", Path.cwd().resolve())\n",
    "print(\"REPO_ROOT:\", ROOT)\n",
    "print(\"src exists:\", (ROOT / \"src\").exists())\n",
    "\n",
    "print(\"cfg.book_txt_path:\", cfg.book_txt_path)\n",
    "print(\"exists:\", cfg.book_txt_path.exists())\n",
    "print(\"absolute:\", cfg.book_txt_path.resolve())\n",
    "print(\"cfg.book_dir:\", cfg.book_dir, \"exists:\", cfg.book_dir.exists())\n",
    "print(\"cfg.pages_dir:\", cfg.pages_dir, \"exists:\", cfg.pages_dir.exists())\n",
    "\n",
    "# Проверка напрямую системным вызовом\n",
    "print(\"os.path.exists:\", os.path.exists(str(cfg.book_txt_path)))\n",
    "\n",
    "from src.chunking_pages import split_text_by_page_markers, write_pages\n",
    "\n",
    "page_files = sorted(cfg.pages_dir.glob(\"page_*.txt\"))\n",
    "\n",
    "if page_files and not cfg.rebuild_pages:\n",
    "    print(f\"Pages already exist: {len(page_files)} files. Set cfg.rebuild_pages=True to rebuild.\")\n",
    "else:\n",
    "    if cfg.rebuild_pages and page_files:\n",
    "        for fp in page_files:\n",
    "            fp.unlink()\n",
    "    if cfg.book_txt_path.exists():\n",
    "        book_path = cfg.book_txt_path\n",
    "    elif cfg.book_md_path.exists():\n",
    "        book_path = cfg.book_md_path\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Book not found: expected {cfg.book_txt_path} or {cfg.book_md_path}.\"\n",
    "        )\n",
    "    text = book_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    pages = split_text_by_page_markers(text)\n",
    "    page_numbers = sorted({p for (p, _) in pages})\n",
    "    print(\"pages count:\", len(page_numbers))\n",
    "    print(\"min page:\", page_numbers[0], \"max page:\", page_numbers[-1])\n",
    "    print(\"first 20:\", page_numbers[:20])\n",
    "    print(\"last 20:\", page_numbers[-20:])\n",
    "    write_pages(pages, cfg.pages_dir)\n",
    "    print(f\"Written {len(pages)} pages to {cfg.pages_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af22084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.chunking_pages import load_page_chunks\n",
    "\n",
    "chunks = load_page_chunks(cfg.book_id, cfg.pages_dir)\n",
    "chunks_df = pd.DataFrame([c.__dict__ for c in chunks])\n",
    "pages_df = chunks_df[[\"page\", \"text\"]].copy()\n",
    "\n",
    "lengths = [len(str(t)) for t in chunks_df[\"text\"].tolist()]\n",
    "if lengths:\n",
    "    arr = np.array(lengths)\n",
    "    print(\"page text lengths (chars):\")\n",
    "    print(\"  min:\", int(arr.min()))\n",
    "    print(\"  p50:\", int(np.percentile(arr, 50)))\n",
    "    print(\"  p90:\", int(np.percentile(arr, 90)))\n",
    "    print(\"  p99:\", int(np.percentile(arr, 99)))\n",
    "    print(\"  max:\", int(arr.max()))\n",
    "    print(\"top 10 lengths:\", sorted(lengths, reverse=True)[:10])\n",
    "    bins = [0, 500, 1000, 1500, 2000, 2500, 3000, 4000, 5000, 8000, 12000, 20000]\n",
    "    print(\"length histogram (chars):\")\n",
    "    for i in range(len(bins) - 1):\n",
    "        lo = bins[i]\n",
    "        hi = bins[i + 1] - 1\n",
    "        count = int(((arr >= lo) & (arr <= hi)).sum())\n",
    "        print(f\"  {lo}-{hi}: {count}\")\n",
    "\n",
    "chunks_df.head(), len(chunks_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ec18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_csv = cfg.pages_csv_path\n",
    "pages_df.to_csv(pages_csv, index=False)\n",
    "print(\"saved:\", pages_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea982bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunks_df.to_dict(orient=\"records\")\n",
    "chunks_df.head(), len(chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retrievers.bm25 import build_bm25_index, load_bm25, save_bm25\n",
    "\n",
    "bm25_path = cfg.bm25_index_path\n",
    "if bm25_path.exists() and not cfg.rebuild_indexes:\n",
    "    bm25_index = load_bm25(bm25_path)\n",
    "    print(\"loaded:\", bm25_path)\n",
    "else:\n",
    "    bm25_index = build_bm25_index(chunks)\n",
    "    save_bm25(bm25_index, bm25_path)\n",
    "    print(\"saved:\", bm25_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb00f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embeddings import EmbeddingConfig\n",
    "\n",
    "VECTOR_BACKEND = (cfg.vector_backend or \"numpy\").lower()\n",
    "if VECTOR_BACKEND not in (\"numpy\", \"qdrant\"):\n",
    "    print(f\"Unknown VECTOR_BACKEND '{VECTOR_BACKEND}', falling back to numpy.\")\n",
    "    VECTOR_BACKEND = \"numpy\"\n",
    "print(\"VECTOR_BACKEND:\", VECTOR_BACKEND)\n",
    "\n",
    "vector_enabled = False\n",
    "vector_index = None\n",
    "emb_cfg = None\n",
    "\n",
    "if not embeddings_enabled:\n",
    "    print(\"Vector index skipped: embeddings_enabled=False (no API key / embedding model).\")\n",
    "else:\n",
    "    emb_cfg = EmbeddingConfig(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        base_url=OPENAI_BASE_URL or None,\n",
    "        model=EMBEDDING_MODEL,\n",
    "        batch_size=cfg.embed_batch_size,\n",
    "    )\n",
    "\n",
    "    if VECTOR_BACKEND == \"qdrant\":\n",
    "        from src.retrievers.vector_qdrant import build_qdrant_index, load_qdrant_meta, QdrantIndex\n",
    "        from src.vectorstores.qdrant_store import QdrantVectorStore, get_client\n",
    "\n",
    "        qdrant_url = cfg.qdrant_url\n",
    "        base_collection = cfg.qdrant_collection\n",
    "        meta_path = cfg.qdrant_meta_path\n",
    "        rebuild_qdrant = cfg.rebuild_qdrant or cfg.rebuild_indexes\n",
    "        recreate_qdrant_collection = cfg.recreate_qdrant_collection\n",
    "\n",
    "        print(\"QDRANT_URL:\", qdrant_url)\n",
    "        print(\"QDRANT_COLLECTION_BASE:\", base_collection)\n",
    "        print(\"qdrant_meta_path:\", meta_path)\n",
    "\n",
    "        if not qdrant_url:\n",
    "            print(\"Qdrant URL is empty, skipping vector/hybrid.\")\n",
    "        else:\n",
    "            try:\n",
    "                client = get_client(qdrant_url)\n",
    "                client.get_collections()\n",
    "                qdrant_ready = True\n",
    "            except Exception as exc:\n",
    "                print(\"Qdrant unavailable, skipping vector/hybrid:\", exc)\n",
    "                qdrant_ready = False\n",
    "\n",
    "            if qdrant_ready:\n",
    "                page_text_by_page = {int(ch[\"page\"]): str(ch.get(\"text\") or \"\") for ch in chunks}\n",
    "                page_chunk_id_by_page = {int(ch[\"page\"]): str(ch.get(\"chunk_id\", f\"p{ch['page']}\")) for ch in chunks}\n",
    "\n",
    "                if meta_path.exists() and not rebuild_qdrant and not recreate_qdrant_collection:\n",
    "                    meta = load_qdrant_meta(meta_path)\n",
    "                    collection_name = meta.get(\"collection\")\n",
    "                    print(\"loaded:\", meta_path)\n",
    "                    print(\"QDRANT_COLLECTION:\", collection_name)\n",
    "\n",
    "                    if meta.get(\"embedding_model\") != EMBEDDING_MODEL:\n",
    "                        print(\"Qdrant meta embedding_model mismatch; rebuilding.\")\n",
    "                        rebuild_qdrant = True\n",
    "\n",
    "                    params = meta.get(\"subchunk_params\", {})\n",
    "                    if params.get(\"max_chars\") != cfg.subchunk_max_chars or params.get(\"overlap\") != cfg.subchunk_overlap:\n",
    "                        print(\"Qdrant meta subchunk params mismatch; rebuilding.\")\n",
    "                        rebuild_qdrant = True\n",
    "\n",
    "                    if not rebuild_qdrant and collection_name:\n",
    "                        if not client.collection_exists(collection_name):\n",
    "                            print(\"Qdrant collection missing; rebuilding.\")\n",
    "                            rebuild_qdrant = True\n",
    "\n",
    "                    if not rebuild_qdrant and collection_name:\n",
    "                        print(\"Using existing Qdrant collection.\")\n",
    "                        store = QdrantVectorStore(client=client, collection=collection_name)\n",
    "                        vector_index = QdrantIndex(\n",
    "                            store=store,\n",
    "                            emb_cfg=emb_cfg,\n",
    "                            page_text_by_page=page_text_by_page,\n",
    "                            page_chunk_id_by_page=page_chunk_id_by_page,\n",
    "                            top_k_subchunks=cfg.dense_top_k_subchunks,\n",
    "                            subchunk_max_chars=cfg.subchunk_max_chars,\n",
    "                            subchunk_overlap=cfg.subchunk_overlap,\n",
    "                            snippet_chars=cfg.demo_snippet_chars,\n",
    "                        )\n",
    "                        vector_enabled = True\n",
    "\n",
    "                if not vector_enabled:\n",
    "                    try:\n",
    "                        print(\"Building Qdrant index...\")\n",
    "                        vector_index, meta = build_qdrant_index(\n",
    "                            client=client,\n",
    "                            base_collection=base_collection,\n",
    "                            book_id=cfg.book_id,\n",
    "                            pages=chunks,\n",
    "                            emb_cfg=emb_cfg,\n",
    "                            subchunk_max_chars=cfg.subchunk_max_chars,\n",
    "                            subchunk_overlap=cfg.subchunk_overlap,\n",
    "                            recreate_collection=recreate_qdrant_collection,\n",
    "                            batch_size=cfg.embed_batch_size,\n",
    "                            snippet_chars=cfg.demo_snippet_chars,\n",
    "                            top_k_subchunks=cfg.dense_top_k_subchunks,\n",
    "                            meta_path=meta_path,\n",
    "                        )\n",
    "                        print(\"QDRANT_COLLECTION:\", meta.get(\"collection\"))\n",
    "                        print(\"saved:\", meta_path)\n",
    "                        vector_enabled = True\n",
    "                    except Exception as exc:\n",
    "                        print(\"Qdrant index build failed, skipping vector/hybrid:\", exc)\n",
    "                        vector_enabled = False\n",
    "\n",
    "    else:\n",
    "        from src.retrievers.vector_numpy import build_vector_index, load_vector_index, save_vector_index\n",
    "\n",
    "        vec_emb_path = cfg.vector_emb_path\n",
    "        vec_meta_path = cfg.vector_meta_path\n",
    "\n",
    "        if vec_emb_path.exists() and vec_meta_path.exists() and not cfg.rebuild_indexes:\n",
    "            vector_index = load_vector_index(vec_emb_path, vec_meta_path)\n",
    "            print(\"loaded:\", vec_emb_path)\n",
    "            print(\"loaded:\", vec_meta_path)\n",
    "        else:\n",
    "            vector_index = build_vector_index(\n",
    "                chunks,\n",
    "                emb_cfg,\n",
    "                subchunk_max_chars=cfg.subchunk_max_chars,\n",
    "                subchunk_overlap=cfg.subchunk_overlap,\n",
    "                snippet_chars=cfg.demo_snippet_chars,\n",
    "            )\n",
    "            save_vector_index(vector_index, vec_emb_path, vec_meta_path)\n",
    "            print(\"saved:\", vec_emb_path)\n",
    "            print(\"saved:\", vec_meta_path)\n",
    "\n",
    "        vector_enabled = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hits(hits, max_chars: int) -> None:\n",
    "    for h in hits:\n",
    "        snippet = \" \".join((h.get(\"text\") or \"\").split())\n",
    "        if len(snippet) > max_chars:\n",
    "            snippet = snippet[:max_chars].rstrip() + \"...\"\n",
    "        print(f\"- page={h['page']} score={h['score']:.4f} | {snippet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from src.retrieval_pipeline import retrieve, build_context\n",
    "from src.eval import evaluate_questions\n",
    "\n",
    "# bm25_index already built earlier\n",
    "# vector_index and emb_cfg exist only if vector_enabled=True\n",
    "\n",
    "def run_retrieve(query: str, mode: str):\n",
    "    return retrieve(\n",
    "        query=query,\n",
    "        mode=mode,\n",
    "        top_k=cfg.top_k,\n",
    "        bm25=bm25_index,\n",
    "        vector=(vector_index if vector_enabled else None),\n",
    "        emb_cfg=(emb_cfg if vector_enabled else None),\n",
    "        vector_top_k=cfg.vector_top_k,\n",
    "        vector_subchunk_k=cfg.dense_top_k_subchunks,\n",
    "        rrf_k=cfg.rrf_k,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de4408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_modes = [\"bm25\"]\n",
    "if vector_enabled:\n",
    "    demo_modes += [\"vector\", \"hybrid\"]\n",
    "\n",
    "for mode in demo_modes:\n",
    "    print(f\"\\nMODE: {mode}\")\n",
    "    for q in cfg.demo_queries:\n",
    "        hits = run_retrieve(q, mode)\n",
    "        print(f\"\\nQUERY: {q}\")\n",
    "        print_hits(hits, cfg.demo_snippet_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4add6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.cost import Pricing, count_tokens, print_cost\n",
    "\n",
    "def _f(x: str, default: float = 0.0) -> float:\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "pricing = Pricing(\n",
    "    embed_price_per_1k_usd=_f(os.getenv(\"EMBED_PRICE_PER_1K_USD\", \"0\")),\n",
    "    chat_in_price_per_1k_usd=_f(os.getenv(\"CHAT_IN_PRICE_PER_1K_USD\", \"0\")),\n",
    "    chat_out_price_per_1k_usd=_f(os.getenv(\"CHAT_OUT_PRICE_PER_1K_USD\", \"0\")),\n",
    ")\n",
    "\n",
    "embedding_tokens_total = None\n",
    "if EMBEDDING_MODEL:\n",
    "    embedding_tokens_total = sum(count_tokens(t, EMBEDDING_MODEL) for t in chunks_df[\"text\"].tolist())\n",
    "\n",
    "if embedding_tokens_total is None:\n",
    "    print(\"Embedding cost skipped: EMBEDDING_MODEL is empty.\")\n",
    "else:\n",
    "    print_cost(embedding_tokens=embedding_tokens_total, pricing=pricing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d3ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Ты — вопрос-ответный ассистент по одной книге.\n",
    "Правила:\n",
    "1) Отвечай ТОЛЬКО на основе предоставленного КОНТЕКСТА (выдержки со страниц).\n",
    "2) Если в контексте нет ответа — скажи: \"В предоставленном контексте ответа нет\".\n",
    "3) Всегда указывай ссылки на страницы в формате \"стр. N\".\n",
    "4) Не выдумывай факты, определения, команды и численные значения.\n",
    "Тон: нейтральный, технический, краткий.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39bc56",
   "metadata": {},
   "source": [
    "## Ask RAG\n",
    "\n",
    "Ниже — пример 2-3 вопросов и ответы системы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc14783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retrieval_pipeline import build_context\n",
    "from src.llm import LLMConfig, generate_answer\n",
    "\n",
    "ask_mode = cfg.retrieval_mode\n",
    "if ask_mode in (\"vector\", \"hybrid\") and not vector_enabled:\n",
    "    print(f\"Ask RAG mode '{ask_mode}' skipped: vector backend unavailable. Falling back to bm25.\")\n",
    "    ask_mode = \"bm25\"\n",
    "\n",
    "rag_contexts = []\n",
    "rag_usages = []\n",
    "\n",
    "llm_cfg = None\n",
    "if chat_enabled:\n",
    "    llm_cfg = LLMConfig(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        base_url=OPENAI_BASE_URL or None,\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=cfg.llm_temperature,\n",
    "        max_tokens=cfg.llm_max_tokens,\n",
    "    )\n",
    "\n",
    "for question in cfg.ask_questions:\n",
    "    print(f\"\\nQUESTION: {question}\")\n",
    "    hits = run_retrieve(question, ask_mode)\n",
    "    print_hits(hits, cfg.demo_snippet_chars)\n",
    "    context = build_context(hits, max_chars=cfg.max_context_chars)\n",
    "    rag_contexts.append(context)\n",
    "\n",
    "    if not chat_enabled:\n",
    "        print(\"LLM generation skipped: chat_enabled=False (no API key / chat model).\")\n",
    "        rag_usages.append({})\n",
    "        continue\n",
    "\n",
    "    answer, usage = generate_answer(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        system_prompt=SYSTEM_PROMPT.strip(),\n",
    "        cfg=llm_cfg,\n",
    "    )\n",
    "    rag_usages.append(usage if isinstance(usage, dict) else {})\n",
    "    print(\"\\nANSWER:\\n\", answer)\n",
    "    if usage:\n",
    "        print(\"\\nUSAGE:\\n\", usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens_total = None\n",
    "completion_tokens_total = None\n",
    "\n",
    "usage_available = False\n",
    "if isinstance(rag_usages, list):\n",
    "    prompt_sum = 0\n",
    "    completion_sum = 0\n",
    "    for usage in rag_usages:\n",
    "        if isinstance(usage, dict) and \"prompt_tokens\" in usage and \"completion_tokens\" in usage:\n",
    "            usage_available = True\n",
    "            prompt_sum += int(usage.get(\"prompt_tokens\", 0))\n",
    "            completion_sum += int(usage.get(\"completion_tokens\", 0))\n",
    "    if usage_available:\n",
    "        prompt_tokens_total = prompt_sum\n",
    "        completion_tokens_total = completion_sum\n",
    "\n",
    "if not usage_available and CHAT_MODEL:\n",
    "    prompt_tokens_total = sum(\n",
    "        count_tokens(\n",
    "            SYSTEM_PROMPT.strip() + \"\\n\" + ctx + \"\\n\" + q,\n",
    "            CHAT_MODEL,\n",
    "        )\n",
    "        for q, ctx in zip(cfg.ask_questions, rag_contexts)\n",
    "    )\n",
    "    completion_tokens_total = 0\n",
    "\n",
    "if prompt_tokens_total is None or completion_tokens_total is None:\n",
    "    print(\"Chat cost skipped: CHAT_MODEL is empty or Ask RAG was not run.\")\n",
    "    print_cost(embedding_tokens=embedding_tokens_total, pricing=pricing)\n",
    "else:\n",
    "    print_cost(\n",
    "        embedding_tokens=embedding_tokens_total,\n",
    "        prompt_tokens=prompt_tokens_total,\n",
    "        completion_tokens=completion_tokens_total,\n",
    "        pricing=pricing,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f7276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = json.load(open(cfg.eval_questions_path, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "page_set = set(int(p) for p in chunks_df[\"page\"].tolist())\n",
    "missing_pages = sorted({int(q[\"gold_page\"]) for q in questions if int(q[\"gold_page\"]) not in page_set})\n",
    "if missing_pages:\n",
    "    raise ValueError(f\"gold_page not found in parsed pages: {missing_pages[:10]}\")\n",
    "\n",
    "modes = [\"bm25\"]\n",
    "if vector_enabled:\n",
    "    modes += [\"vector\", \"hybrid\"]\n",
    "\n",
    "rows = evaluate_questions(\n",
    "    questions=questions,\n",
    "    run_retrieve=run_retrieve,\n",
    "    modes=modes,\n",
    "    ks=list(cfg.eval_ks),\n",
    ")\n",
    "\n",
    "eval_df = pd.DataFrame(rows).sort_values([\"mode\", \"k\"])\n",
    "eval_df\n",
    "\n",
    "out_path = cfg.eval_out_path\n",
    "eval_df.to_csv(out_path, index=False)\n",
    "print(\"saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefa688",
   "metadata": {},
   "source": [
    "## Краткий вывод по таблице метрик (≤150 слов)\n",
    "\n",
    "В таблице видно, что увеличение k с 3 до 5 повышает recall для всех режимов,\n",
    "а MRR отражает качество ранжирования на верхних позициях. BM25 дает\n",
    "устойчивый базовый уровень без зависимости от внешних ключей. При наличии\n",
    "эмбеддингов векторный и гибридный режимы обычно дают более высокий recall@5\n",
    "и MRR@5, что особенно полезно для семантических формулировок вопросов.\n",
    "Гибрид сочетает точность лексического поиска и семантику, поэтому его удобно\n",
    "использовать как основной режим.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c0682",
   "metadata": {},
   "source": [
    "## Общий вывод (≤200 слов)\n",
    "\n",
    "Система обеспечивает воспроизводимый RAG-пайплайн: строгое постраничное\n",
    "разбиение гарантирует корректные ссылки, единый интерфейс ретривера\n",
    "упрощает сравнение режимов, а гибридный поиск сочетает лексическую точность\n",
    "BM25 и семантику эмбеддингов. Встроенная оценка (recall@k и MRR@k)\n",
    "показывает качество поиска, а прозрачный учет стоимости помогает планировать\n",
    "затраты при масштабировании.\n",
    "\n",
    "Идеи улучшения: (1) добавить re-ranker на кросс-энкодере для более точного\n",
    "упорядочивания top-k; (2) заменить in-memory индекс на постоянное векторное\n",
    "хранилище (например, FAISS или Qdrant) с кэшированием эмбеддингов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f57f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8302ba33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901dbd78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61683a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "PROJECT_DIR = Path(\"..\").resolve()\n",
    "ENV_PATH = PROJECT_DIR / \".env\"\n",
    "if ENV_PATH.exists():\n",
    "    load_dotenv(ENV_PATH)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    seed: int = int(os.getenv(\"SEED\", \"42\"))\n",
    "\n",
    "    # paths\n",
    "    project_dir: Path = PROJECT_DIR\n",
    "    data_dir: Path = PROJECT_DIR / \"data\"\n",
    "    indexes_dir: Path = PROJECT_DIR / \"indexes\"\n",
    "    artifacts_dir: Path = PROJECT_DIR / \"artifacts\"\n",
    "\n",
    "    # book/page corpus\n",
    "    book_id: str = os.getenv(\"BOOK_ID\", \"devops_handbook\")\n",
    "    rebuild_pages: bool = os.getenv(\"REBUILD_PAGES\", \"false\").lower() == \"true\"\n",
    "    rebuild_indexes: bool = os.getenv(\"REBUILD_INDEXES\", \"false\").lower() == \"true\"\n",
    "\n",
    "    # vector backend\n",
    "    vector_backend: str = os.getenv(\"VECTOR_BACKEND\", \"numpy\").strip().lower()\n",
    "    qdrant_url: str = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\").strip()\n",
    "    qdrant_collection: str = os.getenv(\"QDRANT_COLLECTION\", \"rag\").strip()\n",
    "    rebuild_qdrant: bool = os.getenv(\"REBUILD_QDRANT\", \"false\").lower() == \"true\"\n",
    "    recreate_qdrant_collection: bool = os.getenv(\"RECREATE_QDRANT_COLLECTION\", \"false\").lower() == \"true\"\n",
    "\n",
    "    # retrieval params\n",
    "    retrieval_mode: str = os.getenv(\"RETRIEVAL_MODE\", \"bm25\")  # bm25 | vector | hybrid\n",
    "    top_k: int = int(os.getenv(\"TOP_K\", \"5\"))\n",
    "    vector_top_k: int = int(os.getenv(\"VECTOR_TOP_K\", \"5\"))\n",
    "    dense_top_k_subchunks: int = int(os.getenv(\"DENSE_TOP_K_SUBCHUNKS\", \"50\"))\n",
    "    subchunk_max_chars: int = int(os.getenv(\"SUBCHUNK_MAX_CHARS\", \"2200\"))\n",
    "    subchunk_overlap: int = int(os.getenv(\"SUBCHUNK_OVERLAP\", \"200\"))\n",
    "    rrf_k: int = int(os.getenv(\"RRF_K\", \"60\"))\n",
    "    max_context_chars: int = int(os.getenv(\"MAX_CONTEXT_CHARS\", \"6000\"))\n",
    "\n",
    "    # embeddings / LLM\n",
    "    embed_batch_size: int = int(os.getenv(\"EMBED_BATCH_SIZE\", \"64\"))\n",
    "    embedding_model: str = os.getenv(\"EMBEDDING_MODEL\", \"\")\n",
    "    chat_model: str = os.getenv(\"CHAT_MODEL\", \"\")\n",
    "    openai_base_url: str = os.getenv(\"OPENAI_BASE_URL\", \"\").strip()\n",
    "    llm_temperature: float = float(os.getenv(\"LLM_TEMPERATURE\", \"0.0\"))\n",
    "    llm_max_tokens: int = int(os.getenv(\"LLM_MAX_TOKENS\", \"600\"))\n",
    "\n",
    "    # evaluation\n",
    "    eval_ks: tuple[int, ...] = (3, 5)\n",
    "\n",
    "    # demo\n",
    "    demo_queries: tuple[str, ...] = (\n",
    "        \"что такое RAG\",\n",
    "        \"индексация\",\n",
    "        \"модель\",\n",
    "    )\n",
    "    ask_questions: tuple[str, ...] = (\n",
    "        \"Как в книге формулируется цель DevOps и почему она важна?\",\n",
    "        \"Какие три пути (The Three Ways) описывает автор и в чем их смысл?\",\n",
    "        \"Что такое value stream mapping и для чего он используется?\",\n",
    "    )\n",
    "    demo_snippet_chars: int = int(os.getenv(\"DEMO_SNIPPET_CHARS\", \"180\"))\n",
    "\n",
    "    @property\n",
    "    def book_dir(self) -> Path:\n",
    "        return self.data_dir / \"books\" / self.book_id\n",
    "\n",
    "    @property\n",
    "    def pages_dir(self) -> Path:\n",
    "        return self.book_dir / \"pages\"\n",
    "\n",
    "    @property\n",
    "    def book_txt_path(self) -> Path:\n",
    "        return self.book_dir / \"book.txt\"\n",
    "\n",
    "    @property\n",
    "    def book_md_path(self) -> Path:\n",
    "        return self.book_dir / \"book.md\"\n",
    "\n",
    "    @property\n",
    "    def pages_csv_path(self) -> Path:\n",
    "        return self.artifacts_dir / \"pages.csv\"\n",
    "\n",
    "    @property\n",
    "    def bm25_index_path(self) -> Path:\n",
    "        return self.indexes_dir / \"bm25.pkl\"\n",
    "\n",
    "    @property\n",
    "    def vector_emb_path(self) -> Path:\n",
    "        return self.indexes_dir / \"vector_embeddings.npy\"\n",
    "\n",
    "    @property\n",
    "    def vector_meta_path(self) -> Path:\n",
    "        return self.indexes_dir / \"vector_meta.json\"\n",
    "\n",
    "    @property\n",
    "    def qdrant_meta_path(self) -> Path:\n",
    "        return self.indexes_dir / f\"qdrant_{self.book_id}_meta.json\"\n",
    "\n",
    "    @property\n",
    "    def eval_questions_path(self) -> Path:\n",
    "        return self.project_dir / \"eval\" / \"questions.json\"\n",
    "\n",
    "    @property\n",
    "    def eval_out_path(self) -> Path:\n",
    "        return self.artifacts_dir / \"retrieval_eval.csv\"\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
    "OPENAI_BASE_URL = cfg.openai_base_url\n",
    "CHAT_MODEL = cfg.chat_model\n",
    "EMBEDDING_MODEL = cfg.embedding_model\n",
    "\n",
    "VECTOR_BACKEND = (cfg.vector_backend or \"numpy\").lower()\n",
    "QDRANT_URL = cfg.qdrant_url\n",
    "QDRANT_COLLECTION = cfg.qdrant_collection\n",
    "\n",
    "embeddings_enabled = bool(OPENAI_API_KEY and EMBEDDING_MODEL)\n",
    "chat_enabled = bool(OPENAI_API_KEY and CHAT_MODEL)\n",
    "llm_enabled = embeddings_enabled or chat_enabled\n",
    "\n",
    "print(\"embeddings_enabled:\", embeddings_enabled)\n",
    "print(\"chat_enabled:\", chat_enabled)\n",
    "print(\"CHAT_MODEL:\", CHAT_MODEL)\n",
    "print(\"EMBEDDING_MODEL:\", EMBEDDING_MODEL)\n",
    "print(\"OPENAI_BASE_URL:\", OPENAI_BASE_URL or \"(default)\")\n",
    "print(\"VECTOR_BACKEND:\", VECTOR_BACKEND)\n",
    "print(\"QDRANT_URL:\", QDRANT_URL or \"(not set)\")\n",
    "print(\"QDRANT_COLLECTION:\", QDRANT_COLLECTION)\n",
    "print(\"SUBCHUNK_MAX_CHARS:\", cfg.subchunk_max_chars)\n",
    "print(\"SUBCHUNK_OVERLAP:\", cfg.subchunk_overlap)\n",
    "print(\"DENSE_TOP_K_SUBCHUNKS:\", cfg.dense_top_k_subchunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e19964",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.indexes_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.pages_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"project_dir:\", cfg.project_dir)\n",
    "print(\"data_dir:\", cfg.data_dir, \"exists:\", cfg.data_dir.exists())\n",
    "print(\"pages_dir:\", cfg.pages_dir, \"exists:\", cfg.pages_dir.exists())\n",
    "print(\"indexes_dir:\", cfg.indexes_dir, \"exists:\", cfg.indexes_dir.exists())\n",
    "print(\"artifacts_dir:\", cfg.artifacts_dir, \"exists:\", cfg.artifacts_dir.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08234e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.chunking_pages import split_text_by_page_markers, write_pages\n",
    "\n",
    "page_files = sorted(cfg.pages_dir.glob(\"page_*.txt\"))\n",
    "\n",
    "if page_files and not cfg.rebuild_pages:\n",
    "    print(f\"Pages already exist: {len(page_files)} files. Set cfg.rebuild_pages=True to rebuild.\")\n",
    "else:\n",
    "    if cfg.rebuild_pages and page_files:\n",
    "        for fp in page_files:\n",
    "            fp.unlink()\n",
    "    if cfg.book_txt_path.exists():\n",
    "        book_path = cfg.book_txt_path\n",
    "    elif cfg.book_md_path.exists():\n",
    "        book_path = cfg.book_md_path\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Book not found: expected {cfg.book_txt_path} or {cfg.book_md_path}.\"\n",
    "        )\n",
    "    text = book_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    pages = split_text_by_page_markers(text)\n",
    "    write_pages(pages, cfg.pages_dir)\n",
    "    print(f\"Written {len(pages)} pages to {cfg.pages_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.chunking_pages import load_page_chunks\n",
    "\n",
    "chunks = load_page_chunks(cfg.book_id, cfg.pages_dir)\n",
    "chunks_df = pd.DataFrame([c.__dict__ for c in chunks])\n",
    "pages_df = chunks_df[[\"page\", \"text\"]].copy()\n",
    "\n",
    "lengths = [len(str(t)) for t in chunks_df[\"text\"].tolist()]\n",
    "if lengths:\n",
    "    arr = np.array(lengths)\n",
    "    print(\"page text lengths (chars):\")\n",
    "    print(\"  min:\", int(arr.min()))\n",
    "    print(\"  p50:\", int(np.percentile(arr, 50)))\n",
    "    print(\"  p90:\", int(np.percentile(arr, 90)))\n",
    "    print(\"  p99:\", int(np.percentile(arr, 99)))\n",
    "    print(\"  max:\", int(arr.max()))\n",
    "    print(\"top 10 lengths:\", sorted(lengths, reverse=True)[:10])\n",
    "    bins = [0, 500, 1000, 1500, 2000, 2500, 3000, 4000, 5000, 8000, 12000, 20000]\n",
    "    print(\"length histogram (chars):\")\n",
    "    for i in range(len(bins) - 1):\n",
    "        lo = bins[i]\n",
    "        hi = bins[i + 1] - 1\n",
    "        count = int(((arr >= lo) & (arr <= hi)).sum())\n",
    "        print(f\"  {lo}-{hi}: {count}\")\n",
    "\n",
    "chunks_df.head(), len(chunks_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_csv = cfg.pages_csv_path\n",
    "pages_df.to_csv(pages_csv, index=False)\n",
    "print(\"saved:\", pages_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd135f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunks_df.to_dict(orient=\"records\")\n",
    "chunks_df.head(), len(chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db281a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retrievers.bm25 import build_bm25_index, load_bm25, save_bm25\n",
    "\n",
    "bm25_path = cfg.bm25_index_path\n",
    "if bm25_path.exists() and not cfg.rebuild_indexes:\n",
    "    bm25_index = load_bm25(bm25_path)\n",
    "    print(\"loaded:\", bm25_path)\n",
    "else:\n",
    "    bm25_index = build_bm25_index(chunks)\n",
    "    save_bm25(bm25_index, bm25_path)\n",
    "    print(\"saved:\", bm25_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embeddings import EmbeddingConfig\n",
    "\n",
    "VECTOR_BACKEND = (cfg.vector_backend or \"numpy\").lower()\n",
    "if VECTOR_BACKEND not in (\"numpy\", \"qdrant\"):\n",
    "    print(f\"Unknown VECTOR_BACKEND '{VECTOR_BACKEND}', falling back to numpy.\")\n",
    "    VECTOR_BACKEND = \"numpy\"\n",
    "print(\"VECTOR_BACKEND:\", VECTOR_BACKEND)\n",
    "\n",
    "vector_enabled = False\n",
    "vector_index = None\n",
    "emb_cfg = None\n",
    "\n",
    "if not embeddings_enabled:\n",
    "    print(\"Vector index skipped: embeddings_enabled=False (no API key / embedding model).\")\n",
    "else:\n",
    "    emb_cfg = EmbeddingConfig(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        base_url=OPENAI_BASE_URL or None,\n",
    "        model=EMBEDDING_MODEL,\n",
    "        batch_size=cfg.embed_batch_size,\n",
    "    )\n",
    "\n",
    "    if VECTOR_BACKEND == \"qdrant\":\n",
    "        from src.retrievers.vector_qdrant import build_qdrant_index, load_qdrant_meta, QdrantIndex\n",
    "        from src.vectorstores.qdrant_store import QdrantVectorStore, get_client\n",
    "\n",
    "        qdrant_url = cfg.qdrant_url\n",
    "        base_collection = cfg.qdrant_collection\n",
    "        meta_path = cfg.qdrant_meta_path\n",
    "        rebuild_qdrant = cfg.rebuild_qdrant or cfg.rebuild_indexes\n",
    "        recreate_qdrant_collection = cfg.recreate_qdrant_collection\n",
    "\n",
    "        print(\"QDRANT_URL:\", qdrant_url)\n",
    "        print(\"QDRANT_COLLECTION_BASE:\", base_collection)\n",
    "        print(\"qdrant_meta_path:\", meta_path)\n",
    "\n",
    "        if not qdrant_url:\n",
    "            print(\"Qdrant URL is empty, skipping vector/hybrid.\")\n",
    "        else:\n",
    "            try:\n",
    "                client = get_client(qdrant_url)\n",
    "                client.get_collections()\n",
    "                qdrant_ready = True\n",
    "            except Exception as exc:\n",
    "                print(\"Qdrant unavailable, skipping vector/hybrid:\", exc)\n",
    "                qdrant_ready = False\n",
    "\n",
    "            if qdrant_ready:\n",
    "                page_text_by_page = {int(ch[\"page\"]): str(ch.get(\"text\") or \"\") for ch in chunks}\n",
    "                page_chunk_id_by_page = {int(ch[\"page\"]): str(ch.get(\"chunk_id\", f\"p{ch['page']}\")) for ch in chunks}\n",
    "\n",
    "                if meta_path.exists() and not rebuild_qdrant and not recreate_qdrant_collection:\n",
    "                    meta = load_qdrant_meta(meta_path)\n",
    "                    collection_name = meta.get(\"collection\")\n",
    "                    print(\"loaded:\", meta_path)\n",
    "                    print(\"QDRANT_COLLECTION:\", collection_name)\n",
    "\n",
    "                    if meta.get(\"embedding_model\") != EMBEDDING_MODEL:\n",
    "                        print(\"Qdrant meta embedding_model mismatch; rebuilding.\")\n",
    "                        rebuild_qdrant = True\n",
    "\n",
    "                    params = meta.get(\"subchunk_params\", {})\n",
    "                    if params.get(\"max_chars\") != cfg.subchunk_max_chars or params.get(\"overlap\") != cfg.subchunk_overlap:\n",
    "                        print(\"Qdrant meta subchunk params mismatch; rebuilding.\")\n",
    "                        rebuild_qdrant = True\n",
    "\n",
    "                    if not rebuild_qdrant and collection_name:\n",
    "                        if not client.collection_exists(collection_name):\n",
    "                            print(\"Qdrant collection missing; rebuilding.\")\n",
    "                            rebuild_qdrant = True\n",
    "\n",
    "                    if not rebuild_qdrant and collection_name:\n",
    "                        print(\"Using existing Qdrant collection.\")\n",
    "                        store = QdrantVectorStore(client=client, collection=collection_name)\n",
    "                        vector_index = QdrantIndex(\n",
    "                            store=store,\n",
    "                            emb_cfg=emb_cfg,\n",
    "                            page_text_by_page=page_text_by_page,\n",
    "                            page_chunk_id_by_page=page_chunk_id_by_page,\n",
    "                            top_k_subchunks=cfg.dense_top_k_subchunks,\n",
    "                            subchunk_max_chars=cfg.subchunk_max_chars,\n",
    "                            subchunk_overlap=cfg.subchunk_overlap,\n",
    "                            snippet_chars=cfg.demo_snippet_chars,\n",
    "                        )\n",
    "                        vector_enabled = True\n",
    "\n",
    "                if not vector_enabled:\n",
    "                    try:\n",
    "                        print(\"Building Qdrant index...\")\n",
    "                        vector_index, meta = build_qdrant_index(\n",
    "                            client=client,\n",
    "                            base_collection=base_collection,\n",
    "                            book_id=cfg.book_id,\n",
    "                            pages=chunks,\n",
    "                            emb_cfg=emb_cfg,\n",
    "                            subchunk_max_chars=cfg.subchunk_max_chars,\n",
    "                            subchunk_overlap=cfg.subchunk_overlap,\n",
    "                            recreate_collection=recreate_qdrant_collection,\n",
    "                            batch_size=cfg.embed_batch_size,\n",
    "                            snippet_chars=cfg.demo_snippet_chars,\n",
    "                            top_k_subchunks=cfg.dense_top_k_subchunks,\n",
    "                            meta_path=meta_path,\n",
    "                        )\n",
    "                        print(\"QDRANT_COLLECTION:\", meta.get(\"collection\"))\n",
    "                        print(\"saved:\", meta_path)\n",
    "                        vector_enabled = True\n",
    "                    except Exception as exc:\n",
    "                        print(\"Qdrant index build failed, skipping vector/hybrid:\", exc)\n",
    "                        vector_enabled = False\n",
    "\n",
    "    else:\n",
    "        from src.retrievers.vector_numpy import build_vector_index, load_vector_index, save_vector_index\n",
    "\n",
    "        vec_emb_path = cfg.vector_emb_path\n",
    "        vec_meta_path = cfg.vector_meta_path\n",
    "\n",
    "        if vec_emb_path.exists() and vec_meta_path.exists() and not cfg.rebuild_indexes:\n",
    "            vector_index = load_vector_index(vec_emb_path, vec_meta_path)\n",
    "            print(\"loaded:\", vec_emb_path)\n",
    "            print(\"loaded:\", vec_meta_path)\n",
    "        else:\n",
    "            vector_index = build_vector_index(\n",
    "                chunks,\n",
    "                emb_cfg,\n",
    "                subchunk_max_chars=cfg.subchunk_max_chars,\n",
    "                subchunk_overlap=cfg.subchunk_overlap,\n",
    "                snippet_chars=cfg.demo_snippet_chars,\n",
    "            )\n",
    "            save_vector_index(vector_index, vec_emb_path, vec_meta_path)\n",
    "            print(\"saved:\", vec_emb_path)\n",
    "            print(\"saved:\", vec_meta_path)\n",
    "\n",
    "        vector_enabled = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hits(hits, max_chars: int) -> None:\n",
    "    for h in hits:\n",
    "        snippet = \" \".join((h.get(\"text\") or \"\").split())\n",
    "        if len(snippet) > max_chars:\n",
    "            snippet = snippet[:max_chars].rstrip() + \"...\"\n",
    "        print(f\"- page={h['page']} score={h['score']:.4f} | {snippet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from src.retrieval_pipeline import retrieve, build_context\n",
    "from src.eval import evaluate_questions\n",
    "\n",
    "# bm25_index already built earlier\n",
    "# vector_index and emb_cfg exist only if vector_enabled=True\n",
    "\n",
    "def run_retrieve(query: str, mode: str):\n",
    "    return retrieve(\n",
    "        query=query,\n",
    "        mode=mode,\n",
    "        top_k=cfg.top_k,\n",
    "        bm25=bm25_index,\n",
    "        vector=(vector_index if vector_enabled else None),\n",
    "        emb_cfg=(emb_cfg if vector_enabled else None),\n",
    "        vector_top_k=cfg.vector_top_k,\n",
    "        vector_subchunk_k=cfg.dense_top_k_subchunks,\n",
    "        rrf_k=cfg.rrf_k,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_modes = [\"bm25\"]\n",
    "if vector_enabled:\n",
    "    demo_modes += [\"vector\", \"hybrid\"]\n",
    "\n",
    "for mode in demo_modes:\n",
    "    print(f\"\\nMODE: {mode}\")\n",
    "    for q in cfg.demo_queries:\n",
    "        hits = run_retrieve(q, mode)\n",
    "        print(f\"\\nQUERY: {q}\")\n",
    "        print_hits(hits, cfg.demo_snippet_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.cost import Pricing, count_tokens, print_cost\n",
    "\n",
    "def _f(x: str, default: float = 0.0) -> float:\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "pricing = Pricing(\n",
    "    embed_price_per_1k_usd=_f(os.getenv(\"EMBED_PRICE_PER_1K_USD\", \"0\")),\n",
    "    chat_in_price_per_1k_usd=_f(os.getenv(\"CHAT_IN_PRICE_PER_1K_USD\", \"0\")),\n",
    "    chat_out_price_per_1k_usd=_f(os.getenv(\"CHAT_OUT_PRICE_PER_1K_USD\", \"0\")),\n",
    ")\n",
    "\n",
    "embedding_tokens_total = None\n",
    "if EMBEDDING_MODEL:\n",
    "    embedding_tokens_total = sum(count_tokens(t, EMBEDDING_MODEL) for t in chunks_df[\"text\"].tolist())\n",
    "\n",
    "if embedding_tokens_total is None:\n",
    "    print(\"Embedding cost skipped: EMBEDDING_MODEL is empty.\")\n",
    "else:\n",
    "    print_cost(embedding_tokens=embedding_tokens_total, pricing=pricing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20009a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Ты — вопрос-ответный ассистент по одной книге.\n",
    "Правила:\n",
    "1) Отвечай ТОЛЬКО на основе предоставленного КОНТЕКСТА (выдержки со страниц).\n",
    "2) Если в контексте нет ответа — скажи: \"В предоставленном контексте ответа нет\".\n",
    "3) Всегда указывай ссылки на страницы в формате \"стр. N\".\n",
    "4) Не выдумывай факты, определения, команды и численные значения.\n",
    "Тон: нейтральный, технический, краткий.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d8264",
   "metadata": {},
   "source": [
    "## Ask RAG\n",
    "\n",
    "Ниже — пример 2-3 вопросов и ответы системы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retrieval_pipeline import build_context\n",
    "from src.llm import LLMConfig, generate_answer\n",
    "\n",
    "ask_mode = cfg.retrieval_mode\n",
    "if ask_mode in (\"vector\", \"hybrid\") and not vector_enabled:\n",
    "    print(f\"Ask RAG mode '{ask_mode}' skipped: vector backend unavailable. Falling back to bm25.\")\n",
    "    ask_mode = \"bm25\"\n",
    "\n",
    "rag_contexts = []\n",
    "rag_usages = []\n",
    "\n",
    "llm_cfg = None\n",
    "if chat_enabled:\n",
    "    llm_cfg = LLMConfig(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        base_url=OPENAI_BASE_URL or None,\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=cfg.llm_temperature,\n",
    "        max_tokens=cfg.llm_max_tokens,\n",
    "    )\n",
    "\n",
    "for question in cfg.ask_questions:\n",
    "    print(f\"\\nQUESTION: {question}\")\n",
    "    hits = run_retrieve(question, ask_mode)\n",
    "    print_hits(hits, cfg.demo_snippet_chars)\n",
    "    context = build_context(hits, max_chars=cfg.max_context_chars)\n",
    "    rag_contexts.append(context)\n",
    "\n",
    "    if not chat_enabled:\n",
    "        print(\"LLM generation skipped: chat_enabled=False (no API key / chat model).\")\n",
    "        rag_usages.append({})\n",
    "        continue\n",
    "\n",
    "    answer, usage = generate_answer(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        system_prompt=SYSTEM_PROMPT.strip(),\n",
    "        cfg=llm_cfg,\n",
    "    )\n",
    "    rag_usages.append(usage if isinstance(usage, dict) else {})\n",
    "    print(\"\\nANSWER:\\n\", answer)\n",
    "    if usage:\n",
    "        print(\"\\nUSAGE:\\n\", usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbec481",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens_total = None\n",
    "completion_tokens_total = None\n",
    "\n",
    "usage_available = False\n",
    "if isinstance(rag_usages, list):\n",
    "    prompt_sum = 0\n",
    "    completion_sum = 0\n",
    "    for usage in rag_usages:\n",
    "        if isinstance(usage, dict) and \"prompt_tokens\" in usage and \"completion_tokens\" in usage:\n",
    "            usage_available = True\n",
    "            prompt_sum += int(usage.get(\"prompt_tokens\", 0))\n",
    "            completion_sum += int(usage.get(\"completion_tokens\", 0))\n",
    "    if usage_available:\n",
    "        prompt_tokens_total = prompt_sum\n",
    "        completion_tokens_total = completion_sum\n",
    "\n",
    "if not usage_available and CHAT_MODEL:\n",
    "    prompt_tokens_total = sum(\n",
    "        count_tokens(\n",
    "            SYSTEM_PROMPT.strip() + \"\\n\" + ctx + \"\\n\" + q,\n",
    "            CHAT_MODEL,\n",
    "        )\n",
    "        for q, ctx in zip(cfg.ask_questions, rag_contexts)\n",
    "    )\n",
    "    completion_tokens_total = 0\n",
    "\n",
    "if prompt_tokens_total is None or completion_tokens_total is None:\n",
    "    print(\"Chat cost skipped: CHAT_MODEL is empty or Ask RAG was not run.\")\n",
    "    print_cost(embedding_tokens=embedding_tokens_total, pricing=pricing)\n",
    "else:\n",
    "    print_cost(\n",
    "        embedding_tokens=embedding_tokens_total,\n",
    "        prompt_tokens=prompt_tokens_total,\n",
    "        completion_tokens=completion_tokens_total,\n",
    "        pricing=pricing,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = json.load(open(cfg.eval_questions_path, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "page_set = set(int(p) for p in chunks_df[\"page\"].tolist())\n",
    "missing_pages = sorted({int(q[\"gold_page\"]) for q in questions if int(q[\"gold_page\"]) not in page_set})\n",
    "if missing_pages:\n",
    "    raise ValueError(f\"gold_page not found in parsed pages: {missing_pages[:10]}\")\n",
    "\n",
    "modes = [\"bm25\"]\n",
    "if vector_enabled:\n",
    "    modes += [\"vector\", \"hybrid\"]\n",
    "\n",
    "rows = evaluate_questions(\n",
    "    questions=questions,\n",
    "    run_retrieve=run_retrieve,\n",
    "    modes=modes,\n",
    "    ks=list(cfg.eval_ks),\n",
    ")\n",
    "\n",
    "eval_df = pd.DataFrame(rows).sort_values([\"mode\", \"k\"])\n",
    "eval_df\n",
    "\n",
    "out_path = cfg.eval_out_path\n",
    "eval_df.to_csv(out_path, index=False)\n",
    "print(\"saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15950712",
   "metadata": {},
   "source": [
    "## Краткий вывод по таблице метрик (≤150 слов)\n",
    "\n",
    "В таблице видно, что увеличение k с 3 до 5 повышает recall для всех режимов,\n",
    "а MRR отражает качество ранжирования на верхних позициях. BM25 дает\n",
    "устойчивый базовый уровень без зависимости от внешних ключей. При наличии\n",
    "эмбеддингов векторный и гибридный режимы обычно дают более высокий recall@5\n",
    "и MRR@5, что особенно полезно для семантических формулировок вопросов.\n",
    "Гибрид сочетает точность лексического поиска и семантику, поэтому его удобно\n",
    "использовать как основной режим.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb362fdc",
   "metadata": {},
   "source": [
    "## Общий вывод (≤200 слов)\n",
    "\n",
    "Система обеспечивает воспроизводимый RAG-пайплайн: строгое постраничное\n",
    "разбиение гарантирует корректные ссылки, единый интерфейс ретривера\n",
    "упрощает сравнение режимов, а гибридный поиск сочетает лексическую точность\n",
    "BM25 и семантику эмбеддингов. Встроенная оценка (recall@k и MRR@k)\n",
    "показывает качество поиска, а прозрачный учет стоимости помогает планировать\n",
    "затраты при масштабировании.\n",
    "\n",
    "Идеи улучшения: (1) добавить re-ranker на кросс-энкодере для более точного\n",
    "упорядочивания top-k; (2) заменить in-memory индекс на постоянное векторное\n",
    "хранилище (например, FAISS или Qdrant) с кэшированием эмбеддингов.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
