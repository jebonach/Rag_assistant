{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65fc778",
   "metadata": {},
   "source": [
    "# RAG baseline notebook\n",
    "\n",
    "This notebook is designed to run top-to-bottom without manual edits.\n",
    "\n",
    "How to run:\n",
    "1) Create venv and install requirements\n",
    "2) Copy `.env.example` -> `.env` and fill keys if you want LLM calls\n",
    "3) Restart kernel & Run all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61683a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    seed: int = int(os.getenv(\"SEED\", \"42\"))\n",
    "\n",
    "    # paths\n",
    "    project_dir: Path = Path(\"..\").resolve()\n",
    "    data_dir: Path = project_dir / \"data\"\n",
    "    indexes_dir: Path = project_dir / \"indexes\"\n",
    "    artifacts_dir: Path = project_dir / \"artifacts\"\n",
    "\n",
    "    # retrieval params (placeholders for now)\n",
    "    top_k: int = 5\n",
    "    chunk_size: int = 800\n",
    "    chunk_overlap: int = 150\n",
    "\n",
    "    txt_encoding: str = \"utf-8\"\n",
    "    search_mode: str = \"bm25\"  # bm25 | vector | hybrid\n",
    "\n",
    "    vector_top_k: int = 5\n",
    "    rrf_k: int = 60\n",
    "    embed_batch_size: int = 64\n",
    "\n",
    "    max_context_chars: int = 6000\n",
    "    llm_temperature: float = 0.0\n",
    "    llm_max_tokens: int = 600\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "cfg = Config()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = cfg.project_dir / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"\")\n",
    "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"\")\n",
    "\n",
    "llm_enabled = bool(OPENAI_API_KEY and (CHAT_MODEL or EMBEDDING_MODEL))\n",
    "print(\"llm_enabled:\", llm_enabled)\n",
    "print(\"CHAT_MODEL:\", CHAT_MODEL)\n",
    "print(\"EMBEDDING_MODEL:\", EMBEDDING_MODEL)\n",
    "print(\"OPENAI_BASE_URL:\", OPENAI_BASE_URL or \"(default)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e19964",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.indexes_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"project_dir:\", cfg.project_dir)\n",
    "print(\"data_dir:\", cfg.data_dir, \"exists:\", cfg.data_dir.exists())\n",
    "print(\"indexes_dir:\", cfg.indexes_dir, \"exists:\", cfg.indexes_dir.exists())\n",
    "print(\"artifacts_dir:\", cfg.artifacts_dir, \"exists:\", cfg.artifacts_dir.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.ingest import read_txt_pages, pages_to_rows\n",
    "\n",
    "txt_path = cfg.data_dir / \"book.txt\"\n",
    "pages = read_txt_pages(txt_path, encoding=cfg.txt_encoding)\n",
    "\n",
    "pages_df = pd.DataFrame(pages_to_rows(pages))\n",
    "pages_df.head(), len(pages_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_csv = cfg.artifacts_dir / \"pages.csv\"\n",
    "pages_df.to_csv(pages_csv, index=False)\n",
    "print(\"saved:\", pages_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd135f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.chunking import pages_to_page_chunks\n",
    "\n",
    "chunks = pages_to_page_chunks(\n",
    "    pages_df.to_dict(orient=\"records\"),\n",
    "    prefix=\"book\",\n",
    "    strict=True,\n",
    ")\n",
    "\n",
    "chunks_df = pd.DataFrame(chunks)\n",
    "chunks_df.head(), len(chunks_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db281a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retrievers.bm25 import build_bm25_index, save_bm25\n",
    "\n",
    "bm25_index = build_bm25_index(chunks_df.to_dict(orient=\"records\"))\n",
    "bm25_path = cfg.indexes_dir / \"bm25.pkl\"\n",
    "save_bm25(bm25_index, bm25_path)\n",
    "print(\"saved:\", bm25_path)\n",
    "\n",
    "# demo queries\n",
    "for q in [\"что такое RAG\", \"индексация\", \"модель\"]:\n",
    "    hits = bm25_index.search(q, k=cfg.top_k)\n",
    "    print(\"\\nQUERY:\", q)\n",
    "    for h in hits:\n",
    "        print(f\"- score={h['score']:.4f} page={h['page']} chunk_id={h['chunk_id']}\")\n",
    "        print(\"  \", h[\"text\"][:180].replace(\"\\n\", \" \"), \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embeddings import EmbeddingConfig\n",
    "from src.retrievers.vector_numpy import build_vector_index, embed_query, save_vector_index\n",
    "\n",
    "if not llm_enabled:\n",
    "    print(\"Vector index skipped: llm_enabled=False (no API key / model).\")\n",
    "else:\n",
    "    emb_cfg = EmbeddingConfig(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        base_url=OPENAI_BASE_URL or None,\n",
    "        model=EMBEDDING_MODEL,\n",
    "        batch_size=cfg.embed_batch_size,\n",
    "    )\n",
    "\n",
    "    vector_index = build_vector_index(chunks_df.to_dict(orient=\"records\"), emb_cfg)\n",
    "\n",
    "    vec_emb_path = cfg.indexes_dir / \"vector_embeddings.npy\"\n",
    "    vec_meta_path = cfg.indexes_dir / \"vector_meta.json\"\n",
    "    save_vector_index(vector_index, vec_emb_path, vec_meta_path)\n",
    "    print(\"saved:\", vec_emb_path)\n",
    "    print(\"saved:\", vec_meta_path)\n",
    "\n",
    "    for q in [\"пример запроса\", \"определение\", \"алгоритм\"]:\n",
    "        qv = embed_query(q, emb_cfg)\n",
    "        hits = vector_index.search(q, qv, k=cfg.vector_top_k)\n",
    "        print(\"\\nQUERY:\", q)\n",
    "        for h in hits:\n",
    "            print(f\"- score={h['score']:.4f} page={h['page']} chunk_id={h['chunk_id']}\")\n",
    "            print(\"  \", h[\"text\"][:180].replace(\"\\n\", \" \"), \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retrievers.hybrid_rrf import rrf_fuse\n",
    "\n",
    "if not llm_enabled:\n",
    "    print(\"Hybrid skipped: llm_enabled=False.\")\n",
    "else:\n",
    "    for q in [\"пример запроса\", \"определение\", \"алгоритм\"]:\n",
    "        bm25_hits = bm25_index.search(q, k=cfg.top_k)\n",
    "        qv = embed_query(q, emb_cfg)\n",
    "        vec_hits = vector_index.search(q, qv, k=cfg.vector_top_k)\n",
    "\n",
    "        fused = rrf_fuse(bm25_hits=bm25_hits, vec_hits=vec_hits, k=cfg.top_k, rrf_k=cfg.rrf_k)\n",
    "\n",
    "        print(\"\\nQUERY:\", q)\n",
    "        for h in fused:\n",
    "            print(f\"- score_rrf={h['score_rrf']:.6f} page={h['page']} chunk_id={h['chunk_id']}\")\n",
    "            print(\"  \", h[\"text\"][:180].replace(\"\\n\", \" \"), \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from src.retrieval_pipeline import retrieve, build_context\n",
    "from src.eval import evaluate_questions\n",
    "\n",
    "# bm25_index уже построен ранее\n",
    "# vector_index и emb_cfg существуют только если llm_enabled=True\n",
    "\n",
    "def run_retrieve(query: str, mode: str):\n",
    "    return retrieve(\n",
    "        query=query,\n",
    "        mode=mode,\n",
    "        top_k=cfg.top_k,\n",
    "        bm25=bm25_index,\n",
    "        vector=(vector_index if llm_enabled else None),\n",
    "        emb_cfg=(emb_cfg if llm_enabled else None),\n",
    "        vector_top_k=getattr(cfg, \"vector_top_k\", None),\n",
    "        rrf_k=getattr(cfg, \"rrf_k\", 60),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20009a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Ты — вопрос-ответный ассистент по одной книге.\n",
    "Правила:\n",
    "1) Отвечай ТОЛЬКО на основе предоставленного КОНТЕКСТА (выдержки со страниц).\n",
    "2) Если в контексте нет ответа — скажи: \"В предоставленном контексте ответа нет\" и кратко уточни, чего не хватает.\n",
    "3) Всегда указывай ссылки на страницы: формат \"стр. N\" (где N — номер страницы из контекста).\n",
    "4) Не выдумывай факты, определения, команды и численные значения.\n",
    "Тон: нейтральный, технический, краткий.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retrieval_pipeline import build_context\n",
    "from src.llm import LLMConfig, generate_answer\n",
    "\n",
    "question = \"Напиши сюда реальный вопрос к книге\"\n",
    "\n",
    "hits = run_retrieve(question, cfg.search_mode)\n",
    "context = build_context(hits, max_chars=cfg.max_context_chars)\n",
    "\n",
    "print(\"MODE:\", cfg.search_mode)\n",
    "print(\"TOP HITS:\", [(h[\"page\"], h[\"chunk_id\"], round(h[\"score\"], 4)) for h in hits[:5]])\n",
    "\n",
    "usage = {}\n",
    "if not llm_enabled or not CHAT_MODEL:\n",
    "    print(\"LLM generation skipped: llm_enabled=False or CHAT_MODEL is empty.\")\n",
    "else:\n",
    "    llm_cfg = LLMConfig(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        base_url=OPENAI_BASE_URL or None,\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=cfg.llm_temperature,\n",
    "        max_tokens=cfg.llm_max_tokens,\n",
    "    )\n",
    "    answer, usage = generate_answer(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        system_prompt=SYSTEM_PROMPT.strip(),\n",
    "        cfg=llm_cfg,\n",
    "    )\n",
    "    print(\"\\nANSWER:\\n\", answer)\n",
    "    print(\"\\nUSAGE:\\n\", usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbec481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.cost import Pricing, count_tokens, print_cost\n",
    "\n",
    "def _f(x: str, default: float = 0.0) -> float:\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "pricing = Pricing(\n",
    "    embed_price_per_1k_usd=_f(os.getenv(\"EMBED_PRICE_PER_1K_USD\", \"0\")),\n",
    "    chat_in_price_per_1k_usd=_f(os.getenv(\"CHAT_IN_PRICE_PER_1K_USD\", \"0\")),\n",
    "    chat_out_price_per_1k_usd=_f(os.getenv(\"CHAT_OUT_PRICE_PER_1K_USD\", \"0\")),\n",
    ")\n",
    "\n",
    "# Примерная оценка токенов на индексацию эмбеддингов (если vector включён):\n",
    "embedding_tokens_est = None\n",
    "if llm_enabled and EMBEDDING_MODEL:\n",
    "    embedding_tokens_est = sum(count_tokens(t, EMBEDDING_MODEL) for t in chunks_df[\"text\"].tolist())\n",
    "\n",
    "# Токены на генерацию (если есть ответ)\n",
    "prompt_tokens_est = None\n",
    "completion_tokens_est = None\n",
    "if llm_enabled and CHAT_MODEL:\n",
    "    prompt_tokens_est = count_tokens(SYSTEM_PROMPT + \"\\n\" + context + \"\\n\" + question, CHAT_MODEL)\n",
    "    # completion_tokens обычно неизвестны до ответа; если usage пришел — лучше взять оттуда.\n",
    "    if isinstance(usage, dict) and \"completion_tokens\" in usage:\n",
    "        completion_tokens_est = int(usage[\"completion_tokens\"])\n",
    "    else:\n",
    "        completion_tokens_est = 0\n",
    "\n",
    "print_cost(\n",
    "    embedding_tokens=embedding_tokens_est,\n",
    "    prompt_tokens=prompt_tokens_est,\n",
    "    completion_tokens=completion_tokens_est,\n",
    "    pricing=pricing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = json.load(open(cfg.project_dir / \"eval\" / \"questions.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "modes = [\"bm25\"]\n",
    "if llm_enabled:\n",
    "    modes += [\"vector\", \"hybrid\"]\n",
    "\n",
    "rows = evaluate_questions(\n",
    "    questions=questions,\n",
    "    run_retrieve=run_retrieve,\n",
    "    modes=modes,\n",
    "    ks=[3, 5],\n",
    ")\n",
    "\n",
    "eval_df = pd.DataFrame(rows).sort_values([\"mode\", \"k\"])\n",
    "eval_df\n",
    "\n",
    "out_path = cfg.artifacts_dir / \"retrieval_eval.csv\"\n",
    "eval_df.to_csv(out_path, index=False)\n",
    "print(\"saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15950712",
   "metadata": {},
   "source": [
    "## Краткий вывод по таблице метрик (≤150 слов)\n",
    "\n",
    "_Заполнить после расчета метрик._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb362fdc",
   "metadata": {},
   "source": [
    "## Общий вывод (≤200 слов)\n",
    "\n",
    "_Укажи преимущества и 2 улучшения._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
