{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d65fc778",
      "metadata": {},
      "source": [
        "# RAG baseline notebook\n",
        "\n",
        "Краткий запуск:\n",
        "1) Создай venv и установи зависимости из requirements.txt\n",
        "2) Скопируй .env.example -> .env и заполни ключи/тарифы при необходимости\n",
        "3) Restart & Run All\n",
        "\n",
        "Настройки:\n",
        "- Меняются в ячейке Config (book_id, top_k, retrieval_mode, флаги rebuild_*)\n",
        "- Модели/ключи и тарифы берутся из .env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b61683a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "    seed: int = int(os.getenv(\"SEED\", \"42\"))\n",
        "\n",
        "    # paths\n",
        "    project_dir: Path = Path(\"..\").resolve()\n",
        "    data_dir: Path = project_dir / \"data\"\n",
        "    indexes_dir: Path = project_dir / \"indexes\"\n",
        "    artifacts_dir: Path = project_dir / \"artifacts\"\n",
        "\n",
        "    # book/page corpus\n",
        "    book_id: str = \"devops_handbook\"\n",
        "    pages_dir: Path = project_dir / \"data\" / \"books\" / book_id / \"pages\"\n",
        "    rebuild_pages: bool = False\n",
        "\n",
        "    # retrieval params\n",
        "    retrieval_mode: str = \"fulltext\"  # fulltext | vector | hybrid\n",
        "    top_k: int = 5\n",
        "    vector_top_k: int = 5\n",
        "    rrf_k: int = 60\n",
        "\n",
        "    # embeddings / LLM\n",
        "    embed_batch_size: int = 64\n",
        "    max_context_chars: int = 6000\n",
        "    llm_temperature: float = 0.0\n",
        "    llm_max_tokens: int = 600\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefb2bf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "env_path = cfg.project_dir / \".env\"\n",
        "if env_path.exists():\n",
        "    load_dotenv(env_path)\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
        "OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"\").strip()\n",
        "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"\").strip()\n",
        "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"\").strip()\n",
        "\n",
        "embeddings_enabled = bool(OPENAI_API_KEY and EMBEDDING_MODEL)\n",
        "chat_enabled = bool(OPENAI_API_KEY and CHAT_MODEL)\n",
        "llm_enabled = embeddings_enabled or chat_enabled\n",
        "\n",
        "print(\"embeddings_enabled:\", embeddings_enabled)\n",
        "print(\"chat_enabled:\", chat_enabled)\n",
        "print(\"CHAT_MODEL:\", CHAT_MODEL)\n",
        "print(\"EMBEDDING_MODEL:\", EMBEDDING_MODEL)\n",
        "print(\"OPENAI_BASE_URL:\", OPENAI_BASE_URL or \"(default)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e19964",
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg.data_dir.mkdir(parents=True, exist_ok=True)\n",
        "cfg.indexes_dir.mkdir(parents=True, exist_ok=True)\n",
        "cfg.artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
        "cfg.pages_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"project_dir:\", cfg.project_dir)\n",
        "print(\"data_dir:\", cfg.data_dir, \"exists:\", cfg.data_dir.exists())\n",
        "print(\"pages_dir:\", cfg.pages_dir, \"exists:\", cfg.pages_dir.exists())\n",
        "print(\"indexes_dir:\", cfg.indexes_dir, \"exists:\", cfg.indexes_dir.exists())\n",
        "print(\"artifacts_dir:\", cfg.artifacts_dir, \"exists:\", cfg.artifacts_dir.exists())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from src.chunking_pages import split_text_by_page_markers, write_pages\n",
        "\n",
        "book_dir = cfg.data_dir / \"books\" / cfg.book_id\n",
        "book_txt = book_dir / \"book.txt\"\n",
        "book_md = book_dir / \"book.md\"\n",
        "page_files = sorted(cfg.pages_dir.glob(\"page_*.txt\"))\n",
        "\n",
        "if page_files and not cfg.rebuild_pages:\n",
        "    print(f\"Pages already exist: {len(page_files)} files. Set cfg.rebuild_pages=True to rebuild.\")\n",
        "else:\n",
        "    if cfg.rebuild_pages and page_files:\n",
        "        for fp in page_files:\n",
        "            fp.unlink()\n",
        "    if book_txt.exists():\n",
        "        book_path = book_txt\n",
        "    elif book_md.exists():\n",
        "        book_path = book_md\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Book not found: expected {book_txt} or {book_md}.\"\n",
        "        )\n",
        "    text = book_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    pages = split_text_by_page_markers(text)\n",
        "    write_pages(pages, cfg.pages_dir)\n",
        "    print(f\"Written {len(pages)} pages to {cfg.pages_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c62c58b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from src.chunking_pages import load_page_chunks\n",
        "\n",
        "chunks = load_page_chunks(cfg.book_id, cfg.pages_dir)\n",
        "chunks_df = pd.DataFrame([c.__dict__ for c in chunks])\n",
        "pages_df = chunks_df[[\"page\", \"text\"]].copy()\n",
        "\n",
        "chunks_df.head(), len(chunks_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ea968d",
      "metadata": {},
      "outputs": [],
      "source": [
        "pages_csv = cfg.artifacts_dir / \"pages.csv\"\n",
        "pages_df.to_csv(pages_csv, index=False)\n",
        "print(\"saved:\", pages_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cd135f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks = chunks_df.to_dict(orient=\"records\")\n",
        "chunks_df.head(), len(chunks_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db281a2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.retrievers.bm25 import build_bm25_index, save_bm25\n",
        "\n",
        "bm25_index = build_bm25_index(chunks)\n",
        "bm25_path = cfg.indexes_dir / \"bm25.pkl\"\n",
        "save_bm25(bm25_index, bm25_path)\n",
        "print(\"saved:\", bm25_path)\n",
        "\n",
        "# demo queries\n",
        "for q in [\"что такое RAG\", \"индексация\", \"модель\"]:\n",
        "    hits = bm25_index.search(q, k=cfg.top_k)\n",
        "    print(\"\\nQUERY:\", q)\n",
        "    for h in hits:\n",
        "        print(f\"- score={h['score']:.4f} page={h['page']} chunk_id={h['chunk_id']}\")\n",
        "        print(\"  \", h[\"text\"][:180].replace(\"\\n\", \" \"), \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3eb7fd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.embeddings import EmbeddingConfig\n",
        "from src.retrievers.vector_numpy import build_vector_index, embed_query, save_vector_index\n",
        "\n",
        "if not embeddings_enabled:\n",
        "    print(\"Vector index skipped: embeddings_enabled=False (no API key / embedding model).\")\n",
        "else:\n",
        "    emb_cfg = EmbeddingConfig(\n",
        "        api_key=OPENAI_API_KEY,\n",
        "        base_url=OPENAI_BASE_URL or None,\n",
        "        model=EMBEDDING_MODEL,\n",
        "        batch_size=cfg.embed_batch_size,\n",
        "    )\n",
        "\n",
        "    vector_index = build_vector_index(chunks, emb_cfg)\n",
        "\n",
        "    vec_emb_path = cfg.indexes_dir / \"vector_embeddings.npy\"\n",
        "    vec_meta_path = cfg.indexes_dir / \"vector_meta.json\"\n",
        "    save_vector_index(vector_index, vec_emb_path, vec_meta_path)\n",
        "    print(\"saved:\", vec_emb_path)\n",
        "    print(\"saved:\", vec_meta_path)\n",
        "\n",
        "    for q in [\"пример запроса\", \"определение\", \"алгоритм\"]:\n",
        "        qv = embed_query(q, emb_cfg)\n",
        "        hits = vector_index.search(q, qv, k=cfg.vector_top_k)\n",
        "        print(\"\\nQUERY:\", q)\n",
        "        for h in hits:\n",
        "            print(f\"- score={h['score']:.4f} page={h['page']} chunk_id={h['chunk_id']}\")\n",
        "            print(\"  \", h[\"text\"][:180].replace(\"\\n\", \" \\n\"), \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e4e786d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.retrievers.hybrid_rrf import rrf_fuse\n",
        "\n",
        "if not embeddings_enabled:\n",
        "    print(\"Hybrid skipped: embeddings_enabled=False.\")\n",
        "else:\n",
        "    for q in [\"пример запроса\", \"определение\", \"алгоритм\"]:\n",
        "        bm25_hits = bm25_index.search(q, k=cfg.top_k)\n",
        "        qv = embed_query(q, emb_cfg)\n",
        "        vec_hits = vector_index.search(q, qv, k=cfg.vector_top_k)\n",
        "\n",
        "        fused = rrf_fuse(bm25_hits=bm25_hits, vec_hits=vec_hits, k=cfg.top_k, rrf_k=cfg.rrf_k)\n",
        "\n",
        "        print(\"\\nQUERY:\", q)\n",
        "        for h in fused:\n",
        "            print(f\"- score_rrf={h['score_rrf']:.6f} page={h['page']} chunk_id={h['chunk_id']}\")\n",
        "            print(\"  \", h[\"text\"][:180].replace(\"\\n\", \" \\n\"), \"...\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0819d523",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from src.retrieval_pipeline import retrieve, build_context\n",
        "from src.eval import evaluate_questions\n",
        "\n",
        "# bm25_index уже построен ранее\n",
        "# vector_index и emb_cfg существуют только если embeddings_enabled=True\n",
        "\n",
        "def run_retrieve(query: str, mode: str):\n",
        "    return retrieve(\n",
        "        query=query,\n",
        "        mode=mode,\n",
        "        top_k=cfg.top_k,\n",
        "        bm25=bm25_index,\n",
        "        vector=(vector_index if embeddings_enabled else None),\n",
        "        emb_cfg=(emb_cfg if embeddings_enabled else None),\n",
        "        vector_top_k=getattr(cfg, \"vector_top_k\", None),\n",
        "        rrf_k=getattr(cfg, \"rrf_k\", 60),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20009a25",
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "Ты — вопрос-ответный ассистент по одной книге.\n",
        "Правила:\n",
        "1) Отвечай ТОЛЬКО на основе предоставленного КОНТЕКСТА (выдержки со страниц).\n",
        "2) Если в контексте нет ответа — скажи: \"В предоставленном контексте ответа нет\" и кратко уточни, чего не хватает.\n",
        "3) Всегда указывай ссылки на страницы: формат \"стр. N\" (где N — номер страницы из контекста).\n",
        "4) Не выдумывай факты, определения, команды и численные значения.\n",
        "Тон: нейтральный, технический, краткий.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d4e5f85",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.retrieval_pipeline import build_context\n",
        "from src.llm import LLMConfig, generate_answer\n",
        "\n",
        "question = \"Напиши сюда реальный вопрос к книге\"\n",
        "\n",
        "mode = cfg.retrieval_mode\n",
        "if mode in (\"vector\", \"hybrid\") and not embeddings_enabled:\n",
        "    print(f\"Retrieval mode '{mode}' skipped: embeddings_enabled=False. Falling back to fulltext.\")\n",
        "    mode = \"fulltext\"\n",
        "\n",
        "hits = run_retrieve(question, mode)\n",
        "context = build_context(hits, max_chars=cfg.max_context_chars)\n",
        "\n",
        "print(\"MODE:\", mode)\n",
        "print(\"TOP HITS:\", [(h[\"page\"], h[\"chunk_id\"], round(h[\"score\"], 4)) for h in hits[:5]])\n",
        "\n",
        "usage = {}\n",
        "if not chat_enabled:\n",
        "    print(\"LLM generation skipped: chat_enabled=False (no API key / chat model).\")\n",
        "else:\n",
        "    llm_cfg = LLMConfig(\n",
        "        api_key=OPENAI_API_KEY,\n",
        "        base_url=OPENAI_BASE_URL or None,\n",
        "        model=CHAT_MODEL,\n",
        "        temperature=cfg.llm_temperature,\n",
        "        max_tokens=cfg.llm_max_tokens,\n",
        "    )\n",
        "    answer, usage = generate_answer(\n",
        "        question=question,\n",
        "        context=context,\n",
        "        system_prompt=SYSTEM_PROMPT.strip(),\n",
        "        cfg=llm_cfg,\n",
        "    )\n",
        "    print(\"\\nANSWER:\\n\", answer)\n",
        "    print(\"\\nUSAGE:\\n\", usage)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccbec481",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from src.cost import Pricing, count_tokens, print_cost\n",
        "\n",
        "def _f(x: str, default: float = 0.0) -> float:\n",
        "    try:\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "pricing = Pricing(\n",
        "    embed_price_per_1k_usd=_f(os.getenv(\"EMBED_PRICE_PER_1K_USD\", \"0\")),\n",
        "    chat_in_price_per_1k_usd=_f(os.getenv(\"CHAT_IN_PRICE_PER_1K_USD\", \"0\")),\n",
        "    chat_out_price_per_1k_usd=_f(os.getenv(\"CHAT_OUT_PRICE_PER_1K_USD\", \"0\")),\n",
        ")\n",
        "\n",
        "# Примерная оценка токенов на индексацию эмбеддингов (если vector включён):\n",
        "embedding_tokens_est = None\n",
        "if llm_enabled and EMBEDDING_MODEL:\n",
        "    embedding_tokens_est = sum(count_tokens(t, EMBEDDING_MODEL) for t in chunks_df[\"text\"].tolist())\n",
        "\n",
        "# Токены на генерацию (если есть ответ)\n",
        "prompt_tokens_est = None\n",
        "completion_tokens_est = None\n",
        "if llm_enabled and CHAT_MODEL:\n",
        "    prompt_tokens_est = count_tokens(SYSTEM_PROMPT + \"\\n\" + context + \"\\n\" + question, CHAT_MODEL)\n",
        "    # completion_tokens обычно неизвестны до ответа; если usage пришел — лучше взять оттуда.\n",
        "    if isinstance(usage, dict) and \"completion_tokens\" in usage:\n",
        "        completion_tokens_est = int(usage[\"completion_tokens\"])\n",
        "    else:\n",
        "        completion_tokens_est = 0\n",
        "\n",
        "print_cost(\n",
        "    embedding_tokens=embedding_tokens_est,\n",
        "    prompt_tokens=prompt_tokens_est,\n",
        "    completion_tokens=completion_tokens_est,\n",
        "    pricing=pricing,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13cd0391",
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = json.load(open(cfg.project_dir / \"eval\" / \"questions.json\", \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "modes = [\"fulltext\"]\n",
        "if embeddings_enabled:\n",
        "    modes += [\"vector\", \"hybrid\"]\n",
        "\n",
        "rows = evaluate_questions(\n",
        "    questions=questions,\n",
        "    run_retrieve=run_retrieve,\n",
        "    modes=modes,\n",
        "    ks=[3, 5],\n",
        ")\n",
        "\n",
        "eval_df = pd.DataFrame(rows).sort_values([\"mode\", \"k\"])\n",
        "eval_df\n",
        "\n",
        "out_path = cfg.artifacts_dir / \"retrieval_eval.csv\"\n",
        "eval_df.to_csv(out_path, index=False)\n",
        "print(\"saved:\", out_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15950712",
      "metadata": {},
      "source": [
        "## Краткий вывод по таблице метрик (≤150 слов)\n",
        "\n",
        "_Заполнить после расчета метрик._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb362fdc",
      "metadata": {},
      "source": [
        "## Общий вывод (≤200 слов)\n",
        "\n",
        "_Укажи преимущества и 2 улучшения._"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}